{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Powerful Benchmarker \u00b6 Google Colab Examples \u00b6 See the examples folder for notebooks that give an overview of how to use this library. Installation \u00b6 pip install powerful-benchmarker Modules that can be benchmarked \u00b6 See the list of default modules . You can add other classes and modules by using the register functionality . Getting started \u00b6 Set default flags \u00b6 The easiest way to get started is to download the example script . Then change the default values for the following flags (each one should be an absolute file path, not a relative path): pytorch_home is where you want to save downloaded pretrained models. dataset_root is where your datasets will be downloaded, or where they are already located. root_experiment_folder is where you want all experiment data to be saved. Try a basic command \u00b6 The following command will run an experiment using the default config files , as well as download the CUB200 dataset into your dataset_root python run.py --experiment_name test1 --dataset { CUB200: { download: True }} If the code runs properly, you'll see training and testing progress like this: Experiment folder format \u00b6 Experiment data is saved in the following format: <root_experiment_folder> |-<experiment_name> |-configs |-<split scheme name> |-saved_models |-saved_csvs |-tensorboard_logs |-meta_logs |-saved_csvs |-tensorboard_logs Here's what's in each subfolder: configs contains the yaml config files necessary to reproduce the experiment. <split scheme name>/save_models contains saved pytorch models for a particular split scheme. (A split scheme simply refers to the way train/val/test splits are formed.) <split scheme name>/saved_csvs contains CSV files with data collected during training. It also contains an SQLite database file with the same data. <split scheme name>/tensorboard_logs contains the same information in <split scheme name>/saved_csvs , but in tensorboard format. meta_logs/saved_csvs contains CSV files for aggregate and ensemble accuracies. It also contains an SQLite database file with the same data. meta_logs/tensorboard contains the same information in meta_logs/save_csvs , but in tensorboard format. View experiment data \u00b6 There are multiple ways to view experiment data: Tensorboard \u00b6 Go to the <experiment_name> folder, and run tensorboard at the command line: tensorboard --logdir = . --port = 12345 Then in your web browser, go to localhost:<port> , where <port> is specified in the tensorboard command. You'll see plots like this: CSV \u00b6 Use any text editor or spreadsheet program to view the csv files that are saved in the saved_csvs folders. SQLite \u00b6 Use DB Browser to open the database files that are saved in the saved_csvs folders. Resume training \u00b6 You can interrupt the program and resume training at a later time: python run.py --experiment_name test1 --resume_training latest You can also resume using the model with the best validation accuracy: python run.py --experiment_name test1 --resume_training best Keep track of changes \u00b6 Let's say you finished training for 100 epochs, and decide you want to train for another 50 epochs, for a total of 150. You would run: python run.py --experiment_name test1 --resume_training latest \\ --num_epochs_train 150 --merge_argparse_when_resuming (The merge_argparse_when_resuming flag tells the code that you want to make changes to the original experiment configuration. If you don't use this flag, then the code will ignore your command line arguments, and use the original configuration. The purpose of this is to avoid accidentally changing configs in the middle of an experiment.) Now in your experiments folder you'll see the original config files, and a new folder starting with resume_training . <root_experiment_folder> |-<experiment_name> |-configs |-resume_training_config_diffs_<underscore delimited numbers> ... This folder contains all differences between the originally saved config files and the parameters that you've specified at the command line. In this particular case, there should just be a single file config_general.yaml with a single line: num_epochs_train: 150 . The underscore delimited numbers in the folder name indicate which models were loaded for each split scheme . For example, let's say you are doing cross validation with 3 folds. The training process has finished 50, 30, and 0 epochs of folds 0, 1, and 2, respectively. You decide to stop training, and resume training with a different batch size. Now the config diff folder will be named resume_training_config_diffs_50_30_0 . Reproduce an experiment \u00b6 To reproduce an experiment, use the --reproduce_results flag, and pass in the path to the experiment folder you want to reproduce: python run.py --reproduce_results <experiment_to_reproduce> \\ --experiment_name <your_experiment_name> This will run an experiment based on the config files in experiment_to_reproduce . You can make modifications to the configuration at the command line, as long as you provide the --merge_argparse_when_resuming flag, so that the code knows you intend on making changes: # reproduce the experiment but use a different number of dataloaders python run.py --reproduce_results <experiment_to_reproduce> \\ --experiment_name <your_experiment_name> \\ --trainer~APPLY~2: { dataloader_num_workers: 16 } \\ --tester~APPLY~2: { dataloader_num_workers: 16 } \\ --merge_argparse_when_resuming For a guide on how to reproduce the results of A Metric Learning Reality Check , see the supplementary material Evaluating on specific splits \u00b6 By default, your model will be saved and evaluated on the validation set every save_interval epochs. To get accuracy for specific splits, use the --splits_to_eval flag and pass in a python-style list of split names: --splits_to_eval [train, test] . To run evaluation only, use the --evaluate or --evaluate_ensemble flag. Cross validation split schemes \u00b6 In this library, splits are not hard-coded into the dataset classes. Instead, train/val/test splits are created by a SplitManager , as specified in the config_dataset file: split_manager : ClassDisjointSplitManager : test_size : 0.5 test_start_idx : 0.5 num_training_partitions : 4 num_training_sets : 4 This particular configuration will set aside the second 50% of classes for the test set. Then the first 50% of classes will be used for 4-fold cross validation, in which the train and val splits are always class-disjoint. Advanced usage \u00b6 Here are some other important features of this library: The powerful command line syntax that allows you to easily override, modify, merge, and delete config options at the command line, and within yaml files. Easy and flexible hyperparameter optimization The ability to add custom modules , without having to delve into the benchmarking code.","title":"Home"},{"location":"#powerful-benchmarker","text":"","title":"Powerful Benchmarker"},{"location":"#google-colab-examples","text":"See the examples folder for notebooks that give an overview of how to use this library.","title":"Google Colab Examples"},{"location":"#installation","text":"pip install powerful-benchmarker","title":"Installation"},{"location":"#modules-that-can-be-benchmarked","text":"See the list of default modules . You can add other classes and modules by using the register functionality .","title":"Modules that can be benchmarked"},{"location":"#getting-started","text":"","title":"Getting started"},{"location":"#set-default-flags","text":"The easiest way to get started is to download the example script . Then change the default values for the following flags (each one should be an absolute file path, not a relative path): pytorch_home is where you want to save downloaded pretrained models. dataset_root is where your datasets will be downloaded, or where they are already located. root_experiment_folder is where you want all experiment data to be saved.","title":"Set default flags"},{"location":"#try-a-basic-command","text":"The following command will run an experiment using the default config files , as well as download the CUB200 dataset into your dataset_root python run.py --experiment_name test1 --dataset { CUB200: { download: True }} If the code runs properly, you'll see training and testing progress like this:","title":"Try a basic command"},{"location":"#experiment-folder-format","text":"Experiment data is saved in the following format: <root_experiment_folder> |-<experiment_name> |-configs |-<split scheme name> |-saved_models |-saved_csvs |-tensorboard_logs |-meta_logs |-saved_csvs |-tensorboard_logs Here's what's in each subfolder: configs contains the yaml config files necessary to reproduce the experiment. <split scheme name>/save_models contains saved pytorch models for a particular split scheme. (A split scheme simply refers to the way train/val/test splits are formed.) <split scheme name>/saved_csvs contains CSV files with data collected during training. It also contains an SQLite database file with the same data. <split scheme name>/tensorboard_logs contains the same information in <split scheme name>/saved_csvs , but in tensorboard format. meta_logs/saved_csvs contains CSV files for aggregate and ensemble accuracies. It also contains an SQLite database file with the same data. meta_logs/tensorboard contains the same information in meta_logs/save_csvs , but in tensorboard format.","title":"Experiment folder format"},{"location":"#view-experiment-data","text":"There are multiple ways to view experiment data:","title":"View experiment data"},{"location":"#tensorboard","text":"Go to the <experiment_name> folder, and run tensorboard at the command line: tensorboard --logdir = . --port = 12345 Then in your web browser, go to localhost:<port> , where <port> is specified in the tensorboard command. You'll see plots like this:","title":"Tensorboard"},{"location":"#csv","text":"Use any text editor or spreadsheet program to view the csv files that are saved in the saved_csvs folders.","title":"CSV"},{"location":"#sqlite","text":"Use DB Browser to open the database files that are saved in the saved_csvs folders.","title":"SQLite"},{"location":"#resume-training","text":"You can interrupt the program and resume training at a later time: python run.py --experiment_name test1 --resume_training latest You can also resume using the model with the best validation accuracy: python run.py --experiment_name test1 --resume_training best","title":"Resume training"},{"location":"#keep-track-of-changes","text":"Let's say you finished training for 100 epochs, and decide you want to train for another 50 epochs, for a total of 150. You would run: python run.py --experiment_name test1 --resume_training latest \\ --num_epochs_train 150 --merge_argparse_when_resuming (The merge_argparse_when_resuming flag tells the code that you want to make changes to the original experiment configuration. If you don't use this flag, then the code will ignore your command line arguments, and use the original configuration. The purpose of this is to avoid accidentally changing configs in the middle of an experiment.) Now in your experiments folder you'll see the original config files, and a new folder starting with resume_training . <root_experiment_folder> |-<experiment_name> |-configs |-resume_training_config_diffs_<underscore delimited numbers> ... This folder contains all differences between the originally saved config files and the parameters that you've specified at the command line. In this particular case, there should just be a single file config_general.yaml with a single line: num_epochs_train: 150 . The underscore delimited numbers in the folder name indicate which models were loaded for each split scheme . For example, let's say you are doing cross validation with 3 folds. The training process has finished 50, 30, and 0 epochs of folds 0, 1, and 2, respectively. You decide to stop training, and resume training with a different batch size. Now the config diff folder will be named resume_training_config_diffs_50_30_0 .","title":"Keep track of changes"},{"location":"#reproduce-an-experiment","text":"To reproduce an experiment, use the --reproduce_results flag, and pass in the path to the experiment folder you want to reproduce: python run.py --reproduce_results <experiment_to_reproduce> \\ --experiment_name <your_experiment_name> This will run an experiment based on the config files in experiment_to_reproduce . You can make modifications to the configuration at the command line, as long as you provide the --merge_argparse_when_resuming flag, so that the code knows you intend on making changes: # reproduce the experiment but use a different number of dataloaders python run.py --reproduce_results <experiment_to_reproduce> \\ --experiment_name <your_experiment_name> \\ --trainer~APPLY~2: { dataloader_num_workers: 16 } \\ --tester~APPLY~2: { dataloader_num_workers: 16 } \\ --merge_argparse_when_resuming For a guide on how to reproduce the results of A Metric Learning Reality Check , see the supplementary material","title":"Reproduce an experiment"},{"location":"#evaluating-on-specific-splits","text":"By default, your model will be saved and evaluated on the validation set every save_interval epochs. To get accuracy for specific splits, use the --splits_to_eval flag and pass in a python-style list of split names: --splits_to_eval [train, test] . To run evaluation only, use the --evaluate or --evaluate_ensemble flag.","title":"Evaluating on specific splits"},{"location":"#cross-validation-split-schemes","text":"In this library, splits are not hard-coded into the dataset classes. Instead, train/val/test splits are created by a SplitManager , as specified in the config_dataset file: split_manager : ClassDisjointSplitManager : test_size : 0.5 test_start_idx : 0.5 num_training_partitions : 4 num_training_sets : 4 This particular configuration will set aside the second 50% of classes for the test set. Then the first 50% of classes will be used for 4-fold cross validation, in which the train and val splits are always class-disjoint.","title":"Cross validation split schemes"},{"location":"#advanced-usage","text":"Here are some other important features of this library: The powerful command line syntax that allows you to easily override, modify, merge, and delete config options at the command line, and within yaml files. Easy and flexible hyperparameter optimization The ability to add custom modules , without having to delve into the benchmarking code.","title":"Advanced usage"},{"location":"cl_syntax/","text":"Command Line Syntax \u00b6 This library comes with a powerful command line syntax that makes it easy to change complex configuration options in a precise fashion. Lists and dictionaries \u00b6 Lists and dictionaries are written at the command line in python form: Example list: --splits_to_eval [ train, val, test ] Example nested dictionary --mining_funcs { tuple_miner: { MultiSimilarityMiner: { epsilon: 0 .1 }}} Merge \u00b6 Consider the following optimizer configuration. optimizers : trunk_optimizer : RMSprop : lr : 0.000001 At the command line, we can change lr to 0.01, and add alpha = 0.95 to the RMSprop parameters: --optimizers { trunk_optimizer: { RMSprop: { lr: 0 .01, alpha: 0 .95 }}} So in effect, the config file now looks like this: optimizers : trunk_optimizer : RMSprop : lr : 0.01 alpha : 0.95 In other words, we specify a dictionary at the command line, using python dictionary syntax. This dictionary is then merged into the one specified in the config file. Thus, adding keys is very straightforward: --optimizers { embedder_optimizer: { Adam: { lr: 0 .01 }}} Now the config file includes a specification for embedder_optimizer : optimizers : trunk_optimizer : RMSprop : lr : 0.000001 embedder_optimizer : Adam : lr : 0.01 But what happens if we try to set the trunk_optimizer to Adam ? --optimizers { trunk_optimizer: { Adam: { lr: 0 .01 }}} Now there's a problem with the config file, because two optimizer types are specified for a single optimizer: optimizers : trunk_optimizer : RMSprop : lr : 0.000001 Adam : lr : 0.01 How can we get around this? By using the Override syntax. Override \u00b6 Overriding simple options requires no special syntax. For example, the following will change save_interval from its default value of 2 to 5: --save_interval 5 However, for complex options (i.e. nested dictionaries) the ~OVERRIDE~ flag is required to avoid merges. Let's consider the same optimizer config file from above: optimizers : trunk_optimizer : RMSprop : lr : 0.000001 To instead use Adam with lr = 0.01 : --optimizers~OVERRIDE~ { trunk_optimizer: { Adam: { lr: 0 .01 }}} Now the config file looks like this: optimizers : trunk_optimizer : Adam : lr : 0.01 The ~OVERRIDE~ flag can be used at any level of the dictionary, which comes in handy for more complex config options. Consider this config file: optimizers : trunk_optimizer : RMSprop : lr : 0.000001 embedder_optimizer : Adam : lr : 0.01 We can make trunk_optimizer use Adam, but leave embedder_optimizer unchanged, by applying the ~OVERRIDE~ flag to trunk_optimizer : --optimizers { trunk_optimizer~OVERRIDE~: { Adam: { lr: 0 .01 }}} Apply \u00b6 Sometimes the merging and override capabilities don't offer enough flexibility. Consider this config file: trainer : MetricLossOnly : dataloader_num_workers : 2 batch_size : 32 If we want to change the batch size using merging: --trainer: { MetricLossOnly: { batch_size: 256 }} There are two problems with this: It's verbose. We only wanted to change batch_size , but we had to write out the name of the trainer . It requires knowledge of the trainer that is being used. So instead, we can use the ~APPLY~ flag: --trainer~APPLY~2: { batch_size: 256 } This syntax means that {batch_size: 256} will be applied to (i.e. merged into) all dictionaries at a depth of 2. So the trainer config now looks like: trainer : MetricLossOnly : dataloader_num_workers : 2 batch_size : 256 Here's another example with optimizers. The starting configuration looks like: optimizers : trunk_optimizer : RMSprop : lr : 0.000001 embedder_optimizer : Adam : lr : 0.01 We can set both learning rates to 0.005: --optimizers~APPLY~3 { lr: 0 .005 } The new config file looks like: optimizers : trunk_optimizer : RMSprop : lr : 0.005 embedder_optimizer : Adam : lr : 0.005 Swap \u00b6 Consider the trainer config file again, but with more of its parameters listed: trainer : MetricLossOnly : iterations_per_epoch : 100 dataloader_num_workers : 2 batch_size : 32 freeze_trunk_batchnorm : True label_hierarchy_level : 0 loss_weights : null set_min_label_to_zero : True Let's say you write your own custom trainer, and it has the same set of initialization parameters. One way to use your custom trainer is to use the ~OVERRIDE~ flag: --trainer~OVERRIDE~ { YourCustomTrainer: { iterations_per_epoch: 100 , \\ dataloader_num_workers: 32 , \\ batch_size: 32 , \\ freeze_trunk_batchnorm: True, \\ label_hierarchy_level: 0 , \\ loss_weights: null, \\ set_min_label_to_zero: True }} Again, this is very verbose, considering we only wanted to change the trainer type. So instead, we can use the ~SWAP~ flag: --trainer~SWAP~1 { YourCustomTrainer: {}} This goes to a dictionary depth of 1, and swaps the only key, MetricLossOnly , with YourCustomTrainer , while leaving everything else unchanged. Now the config file looks like: trainer : YourCustomTrainer : iterations_per_epoch : 100 dataloader_num_workers : 2 batch_size : 32 freeze_trunk_batchnorm : True label_hierarchy_level : 0 loss_weights : null set_min_label_to_zero : True What if there are multiple keys at the specified depth? For example, consider this configuration for data transforms: transforms : train : Resize : size : 256 RandomResizedCrop : scale : 0.16 1 ratio : 0.75 1.33 size : 227 RandomHorizontalFlip : p : 0.5 If we want to swap RandomHorizontalFlip out for RandomVerticalFlip , we need to explicitly indicate the mapping, because there are 2 other keys that could be swapped out ( Resize and RandomResizedCrop ): --transforms~SWAP~2 { RandomHorizontalFlip: RandomVerticalFlip } The new config file contains RandomVerticalFlip in place of RandomHorizontalFlip : transforms : train : Resize : size : 256 RandomResizedCrop : scale : 0.16 1 ratio : 0.75 1.33 size : 227 RandomVerticalFlip : p : 0.5 Delete \u00b6 Consider this models config file: models : trunk : bninception : pretrained : imagenet embedder : MLP : layer_sizes : - 128 Let's replace embedder with Identity() , which is a essentially an empty PyTorch module: --models { embedder~OVERRIDE~ { Identity: {}}} But because embedder has no optimizable parameters, we need to get rid of the embedder_optimizer that is specified in the default config file. We can do this easily with the ~DELETE~ flag: --optimizers { embedder_optimizer~DELETE~: {}}","title":"Command Line Syntax"},{"location":"cl_syntax/#command-line-syntax","text":"This library comes with a powerful command line syntax that makes it easy to change complex configuration options in a precise fashion.","title":"Command Line Syntax"},{"location":"cl_syntax/#lists-and-dictionaries","text":"Lists and dictionaries are written at the command line in python form: Example list: --splits_to_eval [ train, val, test ] Example nested dictionary --mining_funcs { tuple_miner: { MultiSimilarityMiner: { epsilon: 0 .1 }}}","title":"Lists and dictionaries"},{"location":"cl_syntax/#merge","text":"Consider the following optimizer configuration. optimizers : trunk_optimizer : RMSprop : lr : 0.000001 At the command line, we can change lr to 0.01, and add alpha = 0.95 to the RMSprop parameters: --optimizers { trunk_optimizer: { RMSprop: { lr: 0 .01, alpha: 0 .95 }}} So in effect, the config file now looks like this: optimizers : trunk_optimizer : RMSprop : lr : 0.01 alpha : 0.95 In other words, we specify a dictionary at the command line, using python dictionary syntax. This dictionary is then merged into the one specified in the config file. Thus, adding keys is very straightforward: --optimizers { embedder_optimizer: { Adam: { lr: 0 .01 }}} Now the config file includes a specification for embedder_optimizer : optimizers : trunk_optimizer : RMSprop : lr : 0.000001 embedder_optimizer : Adam : lr : 0.01 But what happens if we try to set the trunk_optimizer to Adam ? --optimizers { trunk_optimizer: { Adam: { lr: 0 .01 }}} Now there's a problem with the config file, because two optimizer types are specified for a single optimizer: optimizers : trunk_optimizer : RMSprop : lr : 0.000001 Adam : lr : 0.01 How can we get around this? By using the Override syntax.","title":"Merge"},{"location":"cl_syntax/#override","text":"Overriding simple options requires no special syntax. For example, the following will change save_interval from its default value of 2 to 5: --save_interval 5 However, for complex options (i.e. nested dictionaries) the ~OVERRIDE~ flag is required to avoid merges. Let's consider the same optimizer config file from above: optimizers : trunk_optimizer : RMSprop : lr : 0.000001 To instead use Adam with lr = 0.01 : --optimizers~OVERRIDE~ { trunk_optimizer: { Adam: { lr: 0 .01 }}} Now the config file looks like this: optimizers : trunk_optimizer : Adam : lr : 0.01 The ~OVERRIDE~ flag can be used at any level of the dictionary, which comes in handy for more complex config options. Consider this config file: optimizers : trunk_optimizer : RMSprop : lr : 0.000001 embedder_optimizer : Adam : lr : 0.01 We can make trunk_optimizer use Adam, but leave embedder_optimizer unchanged, by applying the ~OVERRIDE~ flag to trunk_optimizer : --optimizers { trunk_optimizer~OVERRIDE~: { Adam: { lr: 0 .01 }}}","title":"Override"},{"location":"cl_syntax/#apply","text":"Sometimes the merging and override capabilities don't offer enough flexibility. Consider this config file: trainer : MetricLossOnly : dataloader_num_workers : 2 batch_size : 32 If we want to change the batch size using merging: --trainer: { MetricLossOnly: { batch_size: 256 }} There are two problems with this: It's verbose. We only wanted to change batch_size , but we had to write out the name of the trainer . It requires knowledge of the trainer that is being used. So instead, we can use the ~APPLY~ flag: --trainer~APPLY~2: { batch_size: 256 } This syntax means that {batch_size: 256} will be applied to (i.e. merged into) all dictionaries at a depth of 2. So the trainer config now looks like: trainer : MetricLossOnly : dataloader_num_workers : 2 batch_size : 256 Here's another example with optimizers. The starting configuration looks like: optimizers : trunk_optimizer : RMSprop : lr : 0.000001 embedder_optimizer : Adam : lr : 0.01 We can set both learning rates to 0.005: --optimizers~APPLY~3 { lr: 0 .005 } The new config file looks like: optimizers : trunk_optimizer : RMSprop : lr : 0.005 embedder_optimizer : Adam : lr : 0.005","title":"Apply"},{"location":"cl_syntax/#swap","text":"Consider the trainer config file again, but with more of its parameters listed: trainer : MetricLossOnly : iterations_per_epoch : 100 dataloader_num_workers : 2 batch_size : 32 freeze_trunk_batchnorm : True label_hierarchy_level : 0 loss_weights : null set_min_label_to_zero : True Let's say you write your own custom trainer, and it has the same set of initialization parameters. One way to use your custom trainer is to use the ~OVERRIDE~ flag: --trainer~OVERRIDE~ { YourCustomTrainer: { iterations_per_epoch: 100 , \\ dataloader_num_workers: 32 , \\ batch_size: 32 , \\ freeze_trunk_batchnorm: True, \\ label_hierarchy_level: 0 , \\ loss_weights: null, \\ set_min_label_to_zero: True }} Again, this is very verbose, considering we only wanted to change the trainer type. So instead, we can use the ~SWAP~ flag: --trainer~SWAP~1 { YourCustomTrainer: {}} This goes to a dictionary depth of 1, and swaps the only key, MetricLossOnly , with YourCustomTrainer , while leaving everything else unchanged. Now the config file looks like: trainer : YourCustomTrainer : iterations_per_epoch : 100 dataloader_num_workers : 2 batch_size : 32 freeze_trunk_batchnorm : True label_hierarchy_level : 0 loss_weights : null set_min_label_to_zero : True What if there are multiple keys at the specified depth? For example, consider this configuration for data transforms: transforms : train : Resize : size : 256 RandomResizedCrop : scale : 0.16 1 ratio : 0.75 1.33 size : 227 RandomHorizontalFlip : p : 0.5 If we want to swap RandomHorizontalFlip out for RandomVerticalFlip , we need to explicitly indicate the mapping, because there are 2 other keys that could be swapped out ( Resize and RandomResizedCrop ): --transforms~SWAP~2 { RandomHorizontalFlip: RandomVerticalFlip } The new config file contains RandomVerticalFlip in place of RandomHorizontalFlip : transforms : train : Resize : size : 256 RandomResizedCrop : scale : 0.16 1 ratio : 0.75 1.33 size : 227 RandomVerticalFlip : p : 0.5","title":"Swap"},{"location":"cl_syntax/#delete","text":"Consider this models config file: models : trunk : bninception : pretrained : imagenet embedder : MLP : layer_sizes : - 128 Let's replace embedder with Identity() , which is a essentially an empty PyTorch module: --models { embedder~OVERRIDE~ { Identity: {}}} But because embedder has no optimizable parameters, we need to get rid of the embedder_optimizer that is specified in the default config file. We can do this easily with the ~DELETE~ flag: --optimizers { embedder_optimizer~DELETE~: {}}","title":"Delete"},{"location":"custom/","text":"Adding Custom Modules \u00b6 Register your own classes and modules \u00b6 By default, this library gives you access to various classes in pytorch-metric-learning, torch, torchvision, and pretrainedmodels . Let's say you want to use your own loss function as well as a custom optimizer that isn't available in torch.optim. You can accomplish this by replacing the last two lines of the example script with this: from your_own_loss import YourLossFunction from custom_optimizer import CoolOptimizer r = runner ( ** ( args . __dict__ )) # make the runner aware of them r . register ( \"loss\" , YourLossFunction ) r . register ( \"optimizer\" , CoolOptimizer ) r . run () Now you can access your custom classes just like any other class: loss_funcs : metric_loss : YourLossFunction : optimizers : trunk_optimizer : CoolOptimizer : lr : 0.01 If you have a module containing multiple classes and you want to register all those classes, you can simply register the module: import YourModuleOfLosses r . register ( \"loss\" , YourModuleOfLosses ) Registering your own trainer is a bit more involved, because you need to also create an associated API parser. The name of the api parser should be APIParser<name of your training method> . Here's an example where I make a trainer that extends trainers.MetricLossOnly , and takes in an additional argument foo . If foo is a simple parameter that can be specified directly in a config file, then APIYourTrainer doesn't need to do anything other than exist: from pytorch_metric_learning import trainers from powerful_benchmarker import api_parsers class YourTrainer ( trainers . MetricLossOnly ): def __init__ ( self , foo , ** kwargs ): super () . __init__ ( ** kwargs ) self . foo = foo print ( \"foo = \" , self . foo ) class APIYourTrainer ( api_parsers . BaseAPIParser ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) r = runner ( ** ( args . __dict__ )) r . register ( \"trainer\" , YourTrainer ) r . register ( \"api_parser\" , APIYourTrainer ) r . run () However, if foo is more complex, e.g. it is an object that requires some logic to be created, then you'll want APIYourTrainer to handle that logic, and then add foo to the default_kwargs_trainer dictionary. Check out the code documentation for details on this.","title":"Adding Custom Modules"},{"location":"custom/#adding-custom-modules","text":"","title":"Adding Custom Modules"},{"location":"custom/#register-your-own-classes-and-modules","text":"By default, this library gives you access to various classes in pytorch-metric-learning, torch, torchvision, and pretrainedmodels . Let's say you want to use your own loss function as well as a custom optimizer that isn't available in torch.optim. You can accomplish this by replacing the last two lines of the example script with this: from your_own_loss import YourLossFunction from custom_optimizer import CoolOptimizer r = runner ( ** ( args . __dict__ )) # make the runner aware of them r . register ( \"loss\" , YourLossFunction ) r . register ( \"optimizer\" , CoolOptimizer ) r . run () Now you can access your custom classes just like any other class: loss_funcs : metric_loss : YourLossFunction : optimizers : trunk_optimizer : CoolOptimizer : lr : 0.01 If you have a module containing multiple classes and you want to register all those classes, you can simply register the module: import YourModuleOfLosses r . register ( \"loss\" , YourModuleOfLosses ) Registering your own trainer is a bit more involved, because you need to also create an associated API parser. The name of the api parser should be APIParser<name of your training method> . Here's an example where I make a trainer that extends trainers.MetricLossOnly , and takes in an additional argument foo . If foo is a simple parameter that can be specified directly in a config file, then APIYourTrainer doesn't need to do anything other than exist: from pytorch_metric_learning import trainers from powerful_benchmarker import api_parsers class YourTrainer ( trainers . MetricLossOnly ): def __init__ ( self , foo , ** kwargs ): super () . __init__ ( ** kwargs ) self . foo = foo print ( \"foo = \" , self . foo ) class APIYourTrainer ( api_parsers . BaseAPIParser ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) r = runner ( ** ( args . __dict__ )) r . register ( \"trainer\" , YourTrainer ) r . register ( \"api_parser\" , APIYourTrainer ) r . run () However, if foo is more complex, e.g. it is an object that requires some logic to be created, then you'll want APIYourTrainer to handle that logic, and then add foo to the default_kwargs_trainer dictionary. Check out the code documentation for details on this.","title":"Register your own classes and modules"},{"location":"hyperparams/","text":"Hyperparameter optimization \u00b6 Bayesian optimization \u00b6 This library uses the Ax package for bayesian optimization. Syntax \u00b6 To tune hyperparameters using bayesian optimization: In your config files or at the command line, append ~BAYESIAN~ to any parameter that you want to tune, followed by a lower and upper bound in square brackets. Use ~LOG_BAYESIAN~ for log-scaled parameters, and ~INT_BAYESIAN~ for integer parameters. Specify the number of bayesian optimization iterations with the --bayes_opt_iters command line flag. Here is an example script which uses bayesian optimization to tune 3 hyperparameters for the multi similarity loss. python run.py --bayes_opt_iters 50 \\ --loss_funcs~OVERRIDE~ \\ { metric_loss: { MultiSimilarityLoss: { \\ alpha~LOG_BAYESIAN~: [ 0 .01, 100 ] , \\ beta~LOG_BAYESIAN~: [ 0 .01, 100 ] , \\ base~BAYESIAN~: [ 0 , 1 ]}}} \\ --experiment_name cub_bayes_opt \\ Resume optimization \u00b6 If you stop and want to resume bayesian optimization, simply run run.py with the same experiment_name you were using before. Change optimization bounds \u00b6 You can change the optimization bounds when resuming, by either changing the bounds in the config files or at the command line. The command line is preferable, because any config diffs will be recorded (just like in regular experiments ). If you're using the command line, make sure to also use the --merge_argparse_when_resuming flag. Run reproductions \u00b6 You can run a number of reproductions for the best parameters, so that you can obtain a confidence interval for your results. Use the reproductions flag, and pass in the number of reproductions you want to perform at the end of bayesian optimization. python run.py --bayes_opt_iters 50 --reproductions 10 \\ --experiment_name cub_bayes_opt \\","title":"Hyperparameter Optimization"},{"location":"hyperparams/#hyperparameter-optimization","text":"","title":"Hyperparameter optimization"},{"location":"hyperparams/#bayesian-optimization","text":"This library uses the Ax package for bayesian optimization.","title":"Bayesian optimization"},{"location":"hyperparams/#syntax","text":"To tune hyperparameters using bayesian optimization: In your config files or at the command line, append ~BAYESIAN~ to any parameter that you want to tune, followed by a lower and upper bound in square brackets. Use ~LOG_BAYESIAN~ for log-scaled parameters, and ~INT_BAYESIAN~ for integer parameters. Specify the number of bayesian optimization iterations with the --bayes_opt_iters command line flag. Here is an example script which uses bayesian optimization to tune 3 hyperparameters for the multi similarity loss. python run.py --bayes_opt_iters 50 \\ --loss_funcs~OVERRIDE~ \\ { metric_loss: { MultiSimilarityLoss: { \\ alpha~LOG_BAYESIAN~: [ 0 .01, 100 ] , \\ beta~LOG_BAYESIAN~: [ 0 .01, 100 ] , \\ base~BAYESIAN~: [ 0 , 1 ]}}} \\ --experiment_name cub_bayes_opt \\","title":"Syntax"},{"location":"hyperparams/#resume-optimization","text":"If you stop and want to resume bayesian optimization, simply run run.py with the same experiment_name you were using before.","title":"Resume optimization"},{"location":"hyperparams/#change-optimization-bounds","text":"You can change the optimization bounds when resuming, by either changing the bounds in the config files or at the command line. The command line is preferable, because any config diffs will be recorded (just like in regular experiments ). If you're using the command line, make sure to also use the --merge_argparse_when_resuming flag.","title":"Change optimization bounds"},{"location":"hyperparams/#run-reproductions","text":"You can run a number of reproductions for the best parameters, so that you can obtain a confidence interval for your results. Use the reproductions flag, and pass in the number of reproductions you want to perform at the end of bayesian optimization. python run.py --bayes_opt_iters 50 --reproductions 10 \\ --experiment_name cub_bayes_opt \\","title":"Run reproductions"},{"location":"modules/","text":"Modules Available By Default \u00b6 With this library, objects are created based on the class names and parameters specified in the config files or at the command line. For example, shufflenet_v2_x1_0 is one of the models in the torchvision.models module, so you can use it for your experiment like this: In a config file: models : trunk : shufflenet_v2_x1_0 : pretrained : True At the command line: --models~OVERRIDE~ { trunk: { shufflenet_v2_x1_0: { pretrained: True }}} By default, the following modules are available. Models pretrainedmodels torchvision.models Optimizers torch.optim torch.optim.lr_scheduler Datasets powerful_benchmarker.datasets torchvision.datasets Transforms torchvision.transforms.transforms easy_module_attribute_getter.custom_transforms Losses pytorch_metric_learning.losses torch.nn Miners pytorch_metric_learning.miners Samplers pytorch_metric_learning.samplers torch.utils.data Trainers pytorch_metric_learning.trainers Testers pytorch_metric_learning.testers You can add other classes and modules by using the register functionality .","title":"Modules Available By Default"},{"location":"modules/#modules-available-by-default","text":"With this library, objects are created based on the class names and parameters specified in the config files or at the command line. For example, shufflenet_v2_x1_0 is one of the models in the torchvision.models module, so you can use it for your experiment like this: In a config file: models : trunk : shufflenet_v2_x1_0 : pretrained : True At the command line: --models~OVERRIDE~ { trunk: { shufflenet_v2_x1_0: { pretrained: True }}} By default, the following modules are available. Models pretrainedmodels torchvision.models Optimizers torch.optim torch.optim.lr_scheduler Datasets powerful_benchmarker.datasets torchvision.datasets Transforms torchvision.transforms.transforms easy_module_attribute_getter.custom_transforms Losses pytorch_metric_learning.losses torch.nn Miners pytorch_metric_learning.miners Samplers pytorch_metric_learning.samplers torch.utils.data Trainers pytorch_metric_learning.trainers Testers pytorch_metric_learning.testers You can add other classes and modules by using the register functionality .","title":"Modules Available By Default"},{"location":"yaml_syntax/","text":"Yaml Syntax \u00b6 Config files in this library are yaml files, so they follow standard yaml syntax. But you can also use all of the command line syntax within your config files. For example, here are two config files in the config_general category: default.yaml trainer : MetricLossOnly : iterations_per_epoch : 100 dataloader_num_workers : 2 batch_size : 32 freeze_trunk_batchnorm : True label_hierarchy_level : 0 loss_weights : null set_min_label_to_zero : True num_epochs_train : 1000 save_interval : 2 patience : 9 check_untrained_accuracy : True skip_eval_if_already_done : True skip_ensemble_eval_if_already_done : True save_figures_on_tensorboard : False save_lists_in_db : False override_required_compatible_factories : False with_daml.yaml trainer~SWAP~1 : DeepAdversarialMetricLearning : trainer~APPLY~2 : g_alone_epochs : 0 metric_alone_epochs : 0 g_triplets_per_anchor : 100 loss_weights : metric_loss : 1 synth_loss : 0.1 g_adv_loss : 0.1 g_hard_loss : 0.1 g_reg_loss : 0.1 The with_daml config file contains the special flags ~SWAP~ and ~APPLY~ . This particular config file won't work by itself. However, it can be loaded in conjunction with default at the command line: --config_general [ default, with_daml ] This loads default.yaml , followed by with_daml.yaml . Now the special ~SWAP~ and ~APPLY~ flags will have an effect. Specifically, MetricLossOnly will get swapped out for DeepAdversarialMetricLearning , and then the parameters for DeepAdversarialMetricLearning will be applied to the trainer dictionary. The final config file ends up looking like this: trainer : DeepAdversarialMetricLearning : iterations_per_epoch : 100 dataloader_num_workers : 2 batch_size : 32 freeze_trunk_batchnorm : True label_hierarchy_level : 0 loss_weights : metric_loss : 1 synth_loss : 0.1 g_adv_loss : 0.1 g_hard_loss : 0.1 g_reg_loss : 0.1 set_min_label_to_zero : True g_alone_epochs : 0 metric_alone_epochs : 0 g_triplets_per_anchor : 100 num_epochs_train : 1000 save_interval : 2 patience : 9 check_untrained_accuracy : True skip_eval_if_already_done : True skip_ensemble_eval_if_already_done : True save_figures_on_tensorboard : False save_lists_in_db : False override_required_compatible_factories : False","title":"Yaml Syntax"},{"location":"yaml_syntax/#yaml-syntax","text":"Config files in this library are yaml files, so they follow standard yaml syntax. But you can also use all of the command line syntax within your config files. For example, here are two config files in the config_general category: default.yaml trainer : MetricLossOnly : iterations_per_epoch : 100 dataloader_num_workers : 2 batch_size : 32 freeze_trunk_batchnorm : True label_hierarchy_level : 0 loss_weights : null set_min_label_to_zero : True num_epochs_train : 1000 save_interval : 2 patience : 9 check_untrained_accuracy : True skip_eval_if_already_done : True skip_ensemble_eval_if_already_done : True save_figures_on_tensorboard : False save_lists_in_db : False override_required_compatible_factories : False with_daml.yaml trainer~SWAP~1 : DeepAdversarialMetricLearning : trainer~APPLY~2 : g_alone_epochs : 0 metric_alone_epochs : 0 g_triplets_per_anchor : 100 loss_weights : metric_loss : 1 synth_loss : 0.1 g_adv_loss : 0.1 g_hard_loss : 0.1 g_reg_loss : 0.1 The with_daml config file contains the special flags ~SWAP~ and ~APPLY~ . This particular config file won't work by itself. However, it can be loaded in conjunction with default at the command line: --config_general [ default, with_daml ] This loads default.yaml , followed by with_daml.yaml . Now the special ~SWAP~ and ~APPLY~ flags will have an effect. Specifically, MetricLossOnly will get swapped out for DeepAdversarialMetricLearning , and then the parameters for DeepAdversarialMetricLearning will be applied to the trainer dictionary. The final config file ends up looking like this: trainer : DeepAdversarialMetricLearning : iterations_per_epoch : 100 dataloader_num_workers : 2 batch_size : 32 freeze_trunk_batchnorm : True label_hierarchy_level : 0 loss_weights : metric_loss : 1 synth_loss : 0.1 g_adv_loss : 0.1 g_hard_loss : 0.1 g_reg_loss : 0.1 set_min_label_to_zero : True g_alone_epochs : 0 metric_alone_epochs : 0 g_triplets_per_anchor : 100 num_epochs_train : 1000 save_interval : 2 patience : 9 check_untrained_accuracy : True skip_eval_if_already_done : True skip_ensemble_eval_if_already_done : True save_figures_on_tensorboard : False save_lists_in_db : False override_required_compatible_factories : False","title":"Yaml Syntax"},{"location":"code/aggregators/","text":"Aggregators \u00b6 Given the accuracies of multiple models, an aggregator will return a single value representing the total score. BaseAggregator \u00b6 The base aggregator class. from powerful_benchmarker.aggregators import BaseAggregator BaseAggregator () Methods \u00b6 update_accuracies \u00b6 Updates the internal state with the accuracy for a particular split scheme and splits. update_accuracies ( split_scheme_name , splits_to_eval , hooks , tester ) record_accuracies \u00b6 Saves the internal state to the record keeper (CSV, SQLite, and tensorboard). record_accuracies ( splits_to_eval , meta_record_keeper , hooks , tester ) get_accuracy_and_standard_error \u00b6 If more than one split scheme is used, then the aggregate accuracy and standard error of the mean is returned. Otherwise, just the aggregate accuracy is returned. get_accuracy_and_standard_error ( hooks , tester , meta_record_keeper , num_split_schemes , split_name ) get_aggregate_performance \u00b6 Must be implemented by the child class. get_aggregate_performance ( accuracy_per_split ) MeanAggregator \u00b6 Returns the mean accuracy of multiple models. from powerful_benchmarker.aggregators import MeanAggregator MeanAggregator ()","title":"Aggregators"},{"location":"code/aggregators/#aggregators","text":"Given the accuracies of multiple models, an aggregator will return a single value representing the total score.","title":"Aggregators"},{"location":"code/aggregators/#baseaggregator","text":"The base aggregator class. from powerful_benchmarker.aggregators import BaseAggregator BaseAggregator ()","title":"BaseAggregator"},{"location":"code/aggregators/#methods","text":"","title":"Methods"},{"location":"code/aggregators/#update_accuracies","text":"Updates the internal state with the accuracy for a particular split scheme and splits. update_accuracies ( split_scheme_name , splits_to_eval , hooks , tester )","title":"update_accuracies"},{"location":"code/aggregators/#record_accuracies","text":"Saves the internal state to the record keeper (CSV, SQLite, and tensorboard). record_accuracies ( splits_to_eval , meta_record_keeper , hooks , tester )","title":"record_accuracies"},{"location":"code/aggregators/#get_accuracy_and_standard_error","text":"If more than one split scheme is used, then the aggregate accuracy and standard error of the mean is returned. Otherwise, just the aggregate accuracy is returned. get_accuracy_and_standard_error ( hooks , tester , meta_record_keeper , num_split_schemes , split_name )","title":"get_accuracy_and_standard_error"},{"location":"code/aggregators/#get_aggregate_performance","text":"Must be implemented by the child class. get_aggregate_performance ( accuracy_per_split )","title":"get_aggregate_performance"},{"location":"code/aggregators/#meanaggregator","text":"Returns the mean accuracy of multiple models. from powerful_benchmarker.aggregators import MeanAggregator MeanAggregator ()","title":"MeanAggregator"},{"location":"code/api_parsers/","text":"API Parsers \u00b6 BaseAPIParser \u00b6","title":"API Parsers"},{"location":"code/api_parsers/#api-parsers","text":"","title":"API Parsers"},{"location":"code/api_parsers/#baseapiparser","text":"","title":"BaseAPIParser"},{"location":"code/architectures/","text":"Architectures \u00b6 ListOfModels \u00b6 Turns a list of models into a single model. The specific behavior depends on the init parameters from powerful_benchmarker.architectures.misc_models import ListOfModels ListOfModels ( list_of_models , input_sizes = None , operation_before_concat = None ): Parameters \u00b6 list_of_models : A list of PyTorch models. The list will be converted into torch.nn.ModuleList(list_of_models) . input_sizes : A list of numbers, with the same length as list_of_models . input_sizes[i] is the expected input size of list_of_models[i] . Can also be left as None . operation_before_concat : A function that is applied to the output of list_of_models[i] before being concatenated with the output of the other models. Methods \u00b6 forward \u00b6 The standard PyTorch forward method, but the behavior differs depending on the value of self.input_sizes . If self.input_sizes is None, then each list_of_models[i] will receive the entire input x . If self.input_sizes is a list, then: list_of_models[0] will receive x[:self.input_sizes[0]] list_of_models[1] will receive x[self.input_sizes[0]:self.input_sizes[1]] etc. MLP \u00b6 A very simple multi layer perceptron. from powerful_benchmarker.architectures.misc_models import MLP MLP ( layer_sizes , final_relu = False ) Parameters \u00b6 layer_sizes : A list of numbers, where layer_sizes[0] is the size of the input, and layer_sizes[-1] is the size of the output. final_relu : If True, will apply ReLU to the final layer's input.","title":"Architectures"},{"location":"code/architectures/#architectures","text":"","title":"Architectures"},{"location":"code/architectures/#listofmodels","text":"Turns a list of models into a single model. The specific behavior depends on the init parameters from powerful_benchmarker.architectures.misc_models import ListOfModels ListOfModels ( list_of_models , input_sizes = None , operation_before_concat = None ):","title":"ListOfModels"},{"location":"code/architectures/#parameters","text":"list_of_models : A list of PyTorch models. The list will be converted into torch.nn.ModuleList(list_of_models) . input_sizes : A list of numbers, with the same length as list_of_models . input_sizes[i] is the expected input size of list_of_models[i] . Can also be left as None . operation_before_concat : A function that is applied to the output of list_of_models[i] before being concatenated with the output of the other models.","title":"Parameters"},{"location":"code/architectures/#methods","text":"","title":"Methods"},{"location":"code/architectures/#forward","text":"The standard PyTorch forward method, but the behavior differs depending on the value of self.input_sizes . If self.input_sizes is None, then each list_of_models[i] will receive the entire input x . If self.input_sizes is a list, then: list_of_models[0] will receive x[:self.input_sizes[0]] list_of_models[1] will receive x[self.input_sizes[0]:self.input_sizes[1]] etc.","title":"forward"},{"location":"code/architectures/#mlp","text":"A very simple multi layer perceptron. from powerful_benchmarker.architectures.misc_models import MLP MLP ( layer_sizes , final_relu = False )","title":"MLP"},{"location":"code/architectures/#parameters_1","text":"layer_sizes : A list of numbers, where layer_sizes[0] is the size of the input, and layer_sizes[-1] is the size of the output. final_relu : If True, will apply ReLU to the final layer's input.","title":"Parameters"},{"location":"code/datasets/","text":"Datasets \u00b6 When using powerful-benchmarker: root is set to dataset_root by default transform is set based on the contents of config_transforms , specifically the transforms config option. Cars196 \u00b6 from powerful_benchmarker.datasets import Cars196 Cars196 ( root , transform = None , download = False ) Parameters : root : Where the dataset is located, or where it will be downloaded. transform : The image transform that will be applied. download : Set to True to download the dataset to root . CUB200 \u00b6 from powerful_benchmarker.datasets import CUB200 CUB200 ( root , transform = None , download = False ) Parameters : root : Where the dataset is located, or where it will be downloaded. transform : The image transform that will be applied. download : Set to True to download the dataset to root . StanfordOnlineProducts \u00b6 from powerful_benchmarker.datasets import StanfordOnlineProducts StanfordOnlineProducts ( root , transform = None , download = False ) Parameters : root : Where the dataset is located, or where it will be downloaded. transform : The image transform that will be applied. download : Set to True to download the dataset to root .","title":"Datasets"},{"location":"code/datasets/#datasets","text":"When using powerful-benchmarker: root is set to dataset_root by default transform is set based on the contents of config_transforms , specifically the transforms config option.","title":"Datasets"},{"location":"code/datasets/#cars196","text":"from powerful_benchmarker.datasets import Cars196 Cars196 ( root , transform = None , download = False ) Parameters : root : Where the dataset is located, or where it will be downloaded. transform : The image transform that will be applied. download : Set to True to download the dataset to root .","title":"Cars196"},{"location":"code/datasets/#cub200","text":"from powerful_benchmarker.datasets import CUB200 CUB200 ( root , transform = None , download = False ) Parameters : root : Where the dataset is located, or where it will be downloaded. transform : The image transform that will be applied. download : Set to True to download the dataset to root .","title":"CUB200"},{"location":"code/datasets/#stanfordonlineproducts","text":"from powerful_benchmarker.datasets import StanfordOnlineProducts StanfordOnlineProducts ( root , transform = None , download = False ) Parameters : root : Where the dataset is located, or where it will be downloaded. transform : The image transform that will be applied. download : Set to True to download the dataset to root .","title":"StanfordOnlineProducts"},{"location":"code/ensembles/","text":"Ensembles \u00b6 Ensembles take multiple models and combine them into a single model. BaseEnsemble \u00b6 from powerful_benchmarker.ensembles import BaseEnsemble BaseEnsemble ( normalize_embeddings = True , use_trunk_output = False ) Parameters \u00b6 normalize_embeddings : Perform L2 normalization if True. The specific details are determined by the child class. use_trunk_output : Use the output of the trunk of the ensemble. The specific details are determined by the child class. Methods \u00b6 get_list_of_models \u00b6 Loads models given a list of split scheme folders, and returns a list containing the loaded models. get_list_of_models ( model_factory , model_args , model_name , factory_kwargs , split_folders , device ) create_ensemble_model \u00b6 Returns a single trunk and embedder, given a list of trunks and embedders. Must be implemented by the child class. create_ensemble_model ( list_of_trunks , list_of_embedders ) ConcatenateEmbeddings \u00b6 Returns a trunk that outputs the concatenation of multiple trunk models. Returns an embedder that outputs the concatenation of multiple embedder models. The trunk's output can be passed into the embedder. from powerful_benchmarker.ensembles import ConcatenateEmbeddings ConcatenateEmbeddings ( ** kwargs )","title":"Ensembles"},{"location":"code/ensembles/#ensembles","text":"Ensembles take multiple models and combine them into a single model.","title":"Ensembles"},{"location":"code/ensembles/#baseensemble","text":"from powerful_benchmarker.ensembles import BaseEnsemble BaseEnsemble ( normalize_embeddings = True , use_trunk_output = False )","title":"BaseEnsemble"},{"location":"code/ensembles/#parameters","text":"normalize_embeddings : Perform L2 normalization if True. The specific details are determined by the child class. use_trunk_output : Use the output of the trunk of the ensemble. The specific details are determined by the child class.","title":"Parameters"},{"location":"code/ensembles/#methods","text":"","title":"Methods"},{"location":"code/ensembles/#get_list_of_models","text":"Loads models given a list of split scheme folders, and returns a list containing the loaded models. get_list_of_models ( model_factory , model_args , model_name , factory_kwargs , split_folders , device )","title":"get_list_of_models"},{"location":"code/ensembles/#create_ensemble_model","text":"Returns a single trunk and embedder, given a list of trunks and embedders. Must be implemented by the child class. create_ensemble_model ( list_of_trunks , list_of_embedders )","title":"create_ensemble_model"},{"location":"code/ensembles/#concatenateembeddings","text":"Returns a trunk that outputs the concatenation of multiple trunk models. Returns an embedder that outputs the concatenation of multiple embedder models. The trunk's output can be passed into the embedder. from powerful_benchmarker.ensembles import ConcatenateEmbeddings ConcatenateEmbeddings ( ** kwargs )","title":"ConcatenateEmbeddings"},{"location":"code/factories/","text":"Factories \u00b6","title":"Factories"},{"location":"code/factories/#factories","text":"","title":"Factories"},{"location":"code/runners/","text":"Runners \u00b6 BaseRunner \u00b6 BayesOptRunner \u00b6 SingleExperimentRunner \u00b6","title":"Runners"},{"location":"code/runners/#runners","text":"","title":"Runners"},{"location":"code/runners/#baserunner","text":"","title":"BaseRunner"},{"location":"code/runners/#bayesoptrunner","text":"","title":"BayesOptRunner"},{"location":"code/runners/#singleexperimentrunner","text":"","title":"SingleExperimentRunner"},{"location":"code/split_managers/","text":"Split Managers \u00b6 Split managers take a dataset and form cross-validation splits based on some criteria. BaseSplitManager \u00b6 Split managers must extend this class. from powerful_benchmarker.split_managers import BaseSplitManager BaseSplitManager ( hierarchy_level = 0 , data_and_label_getter_keys = None , labels_attr_name = \"labels\" , label_set_attr_name = None ) Parameters \u00b6 hierarchy_level : For multi-label datasets, this will select the set of labels that correspond to a specific column in the two-dimensional labels. For example, if the dataset has 1000 elements, and each element has 5 labels, then the label set has shape (1000, 5). If hierarchy_level=3 , then labels[:,3] will be used internally, if necessary. data_and_label_getter_keys : If None, then calling dataset[idx] will simply return the raw value of dataset[idx] . Otherwise, it will be assumed that dataset[idx] returns a dictionary, in which case data_and_label_getter_keys should be a list of strings that correspond to the dictionary keys for the data and labels. labels_attr_name : The name of the dataset's attribute that contains the label for each element. label_set_attr_name : The name of the dataset's attribute that contains the set of labels. If None, then the set will be computed using the dataset's list of labels. ClassDisjointSplitManager \u00b6 Creates split schemes such that the trainval/test split has no overlap in class labels, and that each train/val split has no overlap in class labels. Extends IndexSplitManager . from powerful_benchmarker.split_managers import ClassDisjointSplitManager ClassDisjointSplitManager ( ** kwargs ) ClosedSetSplitManager \u00b6 Creates split schemes such that the ratios of class labels in the dataset is reflected in the trainval/test split, and in each train/val split. In other words, if the dataset has two classes, A and B, and 75% of the dataset is class A, then every train/val/test split will consist of roughly 75% class A. Extends IndexSplitManager . from powerful_benchmarker.split_managers import ClosedSetSplitManager ClosedSetSplitManager ( ** kwargs ) IndexSplitManager \u00b6 Creates split schemes based on dataset index. The logic within this class can be adapted for other cases, like splitting based on class label. from powerful_benchmarker.split_managers import IndexSplitManager IndexSplitManager ( num_training_partitions , num_training_sets , test_size = None , test_start_idx = None , shuffle = False , random_seed = None , helper_split_manager = None , ** kwargs ) Parameters \u00b6 num_training_partitions : The number of partitions in the trainval set. For example, if 40% of the dataset is used for the test set, and num_training_partitions = 2 , then 2 partitions of size (60%/2) = 30% will be created in the trainval set. num_training_sets : The number of training and validation sets to create. Each validation set will have a size of one partition. For example, if num_training_partitions = 10 , then the validation set will always be one of those partitions, while the training set will comprise the other 9. test_size : The size of the test size, as a floating point number. For example, test_size=0.4 will make the test set 40% of the dataset. test_start_idx : The location in the dataset where the test set starts. For example, if test_size=0.4 and test_start_idx=0.5 , then the test set will consist of all dataset elements with indices between len(dataset)*0.5 and len(dataset)*0.9 . It will wrap around to the beginning of the dataset if necessary. shuffle : Whether or not to shuffle the dataset when forming the splits. random_seed : A random seed for shuffling. Only applicable if shuffle = True . helper_split_manager : An optional external split manager. If provided, this external split manager will be used to create the trainval/test split. After that, IndexSplitManager will form the train/val splits. MLRCSplitManager \u00b6 The split manager used for A Metric Learning Reality Check . It is basically the same as ClassDisjointSplitManager , but with some differences in rounding and cross-validation fold order. Extends IndexSplitManager . from powerful_benchmarker.split_managers import MLRCSplitManager MLRCSplitManager ( ** kwargs )","title":"Split Managers"},{"location":"code/split_managers/#split-managers","text":"Split managers take a dataset and form cross-validation splits based on some criteria.","title":"Split Managers"},{"location":"code/split_managers/#basesplitmanager","text":"Split managers must extend this class. from powerful_benchmarker.split_managers import BaseSplitManager BaseSplitManager ( hierarchy_level = 0 , data_and_label_getter_keys = None , labels_attr_name = \"labels\" , label_set_attr_name = None )","title":"BaseSplitManager"},{"location":"code/split_managers/#parameters","text":"hierarchy_level : For multi-label datasets, this will select the set of labels that correspond to a specific column in the two-dimensional labels. For example, if the dataset has 1000 elements, and each element has 5 labels, then the label set has shape (1000, 5). If hierarchy_level=3 , then labels[:,3] will be used internally, if necessary. data_and_label_getter_keys : If None, then calling dataset[idx] will simply return the raw value of dataset[idx] . Otherwise, it will be assumed that dataset[idx] returns a dictionary, in which case data_and_label_getter_keys should be a list of strings that correspond to the dictionary keys for the data and labels. labels_attr_name : The name of the dataset's attribute that contains the label for each element. label_set_attr_name : The name of the dataset's attribute that contains the set of labels. If None, then the set will be computed using the dataset's list of labels.","title":"Parameters"},{"location":"code/split_managers/#classdisjointsplitmanager","text":"Creates split schemes such that the trainval/test split has no overlap in class labels, and that each train/val split has no overlap in class labels. Extends IndexSplitManager . from powerful_benchmarker.split_managers import ClassDisjointSplitManager ClassDisjointSplitManager ( ** kwargs )","title":"ClassDisjointSplitManager"},{"location":"code/split_managers/#closedsetsplitmanager","text":"Creates split schemes such that the ratios of class labels in the dataset is reflected in the trainval/test split, and in each train/val split. In other words, if the dataset has two classes, A and B, and 75% of the dataset is class A, then every train/val/test split will consist of roughly 75% class A. Extends IndexSplitManager . from powerful_benchmarker.split_managers import ClosedSetSplitManager ClosedSetSplitManager ( ** kwargs )","title":"ClosedSetSplitManager"},{"location":"code/split_managers/#indexsplitmanager","text":"Creates split schemes based on dataset index. The logic within this class can be adapted for other cases, like splitting based on class label. from powerful_benchmarker.split_managers import IndexSplitManager IndexSplitManager ( num_training_partitions , num_training_sets , test_size = None , test_start_idx = None , shuffle = False , random_seed = None , helper_split_manager = None , ** kwargs )","title":"IndexSplitManager"},{"location":"code/split_managers/#parameters_1","text":"num_training_partitions : The number of partitions in the trainval set. For example, if 40% of the dataset is used for the test set, and num_training_partitions = 2 , then 2 partitions of size (60%/2) = 30% will be created in the trainval set. num_training_sets : The number of training and validation sets to create. Each validation set will have a size of one partition. For example, if num_training_partitions = 10 , then the validation set will always be one of those partitions, while the training set will comprise the other 9. test_size : The size of the test size, as a floating point number. For example, test_size=0.4 will make the test set 40% of the dataset. test_start_idx : The location in the dataset where the test set starts. For example, if test_size=0.4 and test_start_idx=0.5 , then the test set will consist of all dataset elements with indices between len(dataset)*0.5 and len(dataset)*0.9 . It will wrap around to the beginning of the dataset if necessary. shuffle : Whether or not to shuffle the dataset when forming the splits. random_seed : A random seed for shuffling. Only applicable if shuffle = True . helper_split_manager : An optional external split manager. If provided, this external split manager will be used to create the trainval/test split. After that, IndexSplitManager will form the train/val splits.","title":"Parameters"},{"location":"code/split_managers/#mlrcsplitmanager","text":"The split manager used for A Metric Learning Reality Check . It is basically the same as ClassDisjointSplitManager , but with some differences in rounding and cross-validation fold order. Extends IndexSplitManager . from powerful_benchmarker.split_managers import MLRCSplitManager MLRCSplitManager ( ** kwargs )","title":"MLRCSplitManager"},{"location":"code/utils/","text":"Utils \u00b6 constants \u00b6 dataset_utils \u00b6","title":"Utils"},{"location":"code/utils/#utils","text":"","title":"Utils"},{"location":"code/utils/#constants","text":"","title":"constants"},{"location":"code/utils/#dataset_utils","text":"","title":"dataset_utils"},{"location":"configs/config_dataset/","text":"config_dataset \u00b6 dataset \u00b6 This is the dataset that will be used for training, validation, and testing. Default yaml: dataset : CUB200 : Example command line modification: # Change dataset to Cars196 --dataset~OVERRIDE~ { Cars196: {}} splits_to_eval \u00b6 The names of splits for which accuracy should be computed. Default yaml: splits_to_eval : - val Example command line modification: # Eval on train, val, and test. --splits_to_eval [ train, val, test ] split_manager \u00b6 The split manager determines how the train/val/test splits are formed. Default yaml: split_manager : ClassDisjointSplitManager : test_size : 0.5 test_start_idx : 0.5 num_training_partitions : 4 num_training_sets : 4 hierarchy_level : 0 data_and_label_getter_keys : [ data , label ] Example command line modification: # Change number of training sets to 2, and the test size to 0.3 --split_manager~APPLY~2 { test_size: 0 .3, num_training_sets: 2 }","title":"config_dataset"},{"location":"configs/config_dataset/#config_dataset","text":"","title":"config_dataset"},{"location":"configs/config_dataset/#dataset","text":"This is the dataset that will be used for training, validation, and testing. Default yaml: dataset : CUB200 : Example command line modification: # Change dataset to Cars196 --dataset~OVERRIDE~ { Cars196: {}}","title":"dataset"},{"location":"configs/config_dataset/#splits_to_eval","text":"The names of splits for which accuracy should be computed. Default yaml: splits_to_eval : - val Example command line modification: # Eval on train, val, and test. --splits_to_eval [ train, val, test ]","title":"splits_to_eval"},{"location":"configs/config_dataset/#split_manager","text":"The split manager determines how the train/val/test splits are formed. Default yaml: split_manager : ClassDisjointSplitManager : test_size : 0.5 test_start_idx : 0.5 num_training_partitions : 4 num_training_sets : 4 hierarchy_level : 0 data_and_label_getter_keys : [ data , label ] Example command line modification: # Change number of training sets to 2, and the test size to 0.3 --split_manager~APPLY~2 { test_size: 0 .3, num_training_sets: 2 }","title":"split_manager"},{"location":"configs/config_eval/","text":"config_eval \u00b6 tester \u00b6 The tester computes the accuracy of your model. Default yaml: tester : GlobalEmbeddingSpaceTester : reference_set : compared_to_self normalize_embeddings : True use_trunk_output : False batch_size : 32 dataloader_num_workers : 2 pca : null accuracy_calculator : AccuracyCalculator : label_hierarchy_level : 0 visualizer : {} Example command line modification: # Change batch size to 256 and don't normalize embeddings --tester~APPLY~2 { batch_size: 256 , normalize_embeddings: False } aggregator \u00b6 The aggregator takes the accuracies from all the cross-validation models, and returns a single number to represent the overall performance. Default yaml: aggregator : MeanAggregator : split_to_aggregate : val Example command line modification: # Use your own custom aggregator --aggregator~OVERRIDE~ { YourCustomAggregator: {}} ensemble \u00b6 The ensemble combines the cross-validation models into a single model. Default yaml: ensemble : ConcatenateEmbeddings : normalize_embeddings : True use_trunk_output : False hook_container \u00b6 The hook container contains end-of-testing, end-of-epoch, and end-of-iteration hooks. It also contains a record keeper, for writing and reading to database files. Default yaml: hook_container : HookContainer : primary_metric : mean_average_precision_at_r validation_split_name : val save_models : True Example command line modification: # Change the primary metric to precision_at_1 --hook_container~APPLY~2 { primary_metric: precision_at_1 }","title":"config_eval"},{"location":"configs/config_eval/#config_eval","text":"","title":"config_eval"},{"location":"configs/config_eval/#tester","text":"The tester computes the accuracy of your model. Default yaml: tester : GlobalEmbeddingSpaceTester : reference_set : compared_to_self normalize_embeddings : True use_trunk_output : False batch_size : 32 dataloader_num_workers : 2 pca : null accuracy_calculator : AccuracyCalculator : label_hierarchy_level : 0 visualizer : {} Example command line modification: # Change batch size to 256 and don't normalize embeddings --tester~APPLY~2 { batch_size: 256 , normalize_embeddings: False }","title":"tester"},{"location":"configs/config_eval/#aggregator","text":"The aggregator takes the accuracies from all the cross-validation models, and returns a single number to represent the overall performance. Default yaml: aggregator : MeanAggregator : split_to_aggregate : val Example command line modification: # Use your own custom aggregator --aggregator~OVERRIDE~ { YourCustomAggregator: {}}","title":"aggregator"},{"location":"configs/config_eval/#ensemble","text":"The ensemble combines the cross-validation models into a single model. Default yaml: ensemble : ConcatenateEmbeddings : normalize_embeddings : True use_trunk_output : False","title":"ensemble"},{"location":"configs/config_eval/#hook_container","text":"The hook container contains end-of-testing, end-of-epoch, and end-of-iteration hooks. It also contains a record keeper, for writing and reading to database files. Default yaml: hook_container : HookContainer : primary_metric : mean_average_precision_at_r validation_split_name : val save_models : True Example command line modification: # Change the primary metric to precision_at_1 --hook_container~APPLY~2 { primary_metric: precision_at_1 }","title":"hook_container"},{"location":"configs/config_factories/","text":"config_factories \u00b6 factories \u00b6 Factories determine how objects are constructed, based on parameters in the config files, and parameters generated within the code. Default yaml: factories : model : ModelFactory : {} loss : LossFactory : {} miner : MinerFactory : {} sampler : SamplerFactory : {} optimizer : OptimizerFactory : {} tester : TesterFactory : {} trainer : TrainerFactory : {} transform : TransformFactory : {} split_manager : SplitManagerFactory : {} record_keeper : RecordKeeperFactory : {} hook : HookFactory : {} aggregator : AggregatorFactory : {} ensemble : EnsembleFactory : {} Example command line modification: # Set the base_output_model_size manually --factories { model~APPLY~2: { base_output_model_size: 1024 }}","title":"config_factories"},{"location":"configs/config_factories/#config_factories","text":"","title":"config_factories"},{"location":"configs/config_factories/#factories","text":"Factories determine how objects are constructed, based on parameters in the config files, and parameters generated within the code. Default yaml: factories : model : ModelFactory : {} loss : LossFactory : {} miner : MinerFactory : {} sampler : SamplerFactory : {} optimizer : OptimizerFactory : {} tester : TesterFactory : {} trainer : TrainerFactory : {} transform : TransformFactory : {} split_manager : SplitManagerFactory : {} record_keeper : RecordKeeperFactory : {} hook : HookFactory : {} aggregator : AggregatorFactory : {} ensemble : EnsembleFactory : {} Example command line modification: # Set the base_output_model_size manually --factories { model~APPLY~2: { base_output_model_size: 1024 }}","title":"factories"},{"location":"configs/config_general/","text":"config_general \u00b6 api_parser \u00b6 The API parser controls the experiment by making objects and running trainers and testers. (The API parser is called by the Runner, which is the entry point to the program.) By default, BaseAPIParser is used. If you use a custom trainer, the code will try to use API<name_of_your_trainer> , and if that doesn't exist, it will fall back to BaseAPIParser . However, if you explicitly set the api_parser option, then the one you specify will be used. Default yaml: api_parser : null Example command line modification: --api_parser { YourCustomAPIParser: {}} trainer \u00b6 The trainer trains your model. Default yaml: trainer : MetricLossOnly : iterations_per_epoch : 100 dataloader_num_workers : 2 batch_size : 32 freeze_trunk_batchnorm : True label_hierarchy_level : 0 loss_weights : null set_min_label_to_zero : True Example command line modification: # Swap in a different trainer, but keep the input parameters the same --trainer~SWAP~1 { CascadedEmbeddings: null } num_epochs_train \u00b6 The maximum number of epochs to train for. Default yaml: num_epochs_train : 1000 Example command line modification: --num_epochs_train 100 save_interval \u00b6 Models will be evaluated and saved every save_interval epochs. Default yaml: save_interval : 2 Example command line modification: --save_interval 10 patience \u00b6 Training will end if the validation accuracy stops improving after patience+1 epochs. Default yaml: save_interval : 2 Example command line modification: # Don't use patience at all --patience null check_untrained_accuracy \u00b6 If True , then the tester will compute accuracy for the initial trunk (epoch -1) and initial trunk + embedder (epoch 0). Otherwise, these will be skipped. Default yaml: check_untrained_accuracy : True Example command line modification: --check_untrained_accuracy False skip_eval_if_already_done \u00b6 If True , then the tester will skip evaluation if a split/epoch has already been logged in the log files. If False , then the tester will evaluate a split/epoch regardless of whether it has already been done in the past. Previous logs will be preserved, hence the logs will contain duplicate results, and the most recent version for any split/epoch will be considered the \"official\" value for that split/epoch. Default yaml: skip_eval_if_already_done : True Example command line modification: --skip_eval_if_already_done False skip_ensemble_eval_if_already_done \u00b6 The same as skip_eval_if_already_done , but for ensembles. Default yaml: skip_ensemble_eval_if_already_done : True Example command line modification: --skip_ensemble_eval_if_already_done False log_data_to_tensorboard \u00b6 Set to False if you don't want to log data to tensorboard. You might want to do this if your disk I/O is slow. Default yaml: log_data_to_tensorboard : True Example command line modification: --log_data_to_tensorboard False save_figures_on_tensorboard \u00b6 Use matplotlib to plot things on tensorboard. (Most data doesn't require matplotlib.) Default yaml: save_figures_on_tensorboard : False Example command line modification: --save_figures_on_tensorboard True save_lists_in_db \u00b6 In record-keeper, non-scalar values are saved in the database as json-lists. This setting is False by default, because these lists can sometimes be quite large, causing the database file size to grow quickly. Default yaml: save_lists_in_db : False Example command line modification: --save_lists_in_db True override_required_compatible_factories \u00b6 Each APIParser comes with predefined compatible factories, which are used by default, regardless of what is specified in the factories config option. This allows you to specify a trainer without having to specify all the required factories. However, if you have your own custom factory that you know is compatible, and want to use that instead, you should set this flag to True. Default yaml: override_required_compatible_factories : False Example command line modification: --override_required_compatible_factories True","title":"config_general"},{"location":"configs/config_general/#config_general","text":"","title":"config_general"},{"location":"configs/config_general/#api_parser","text":"The API parser controls the experiment by making objects and running trainers and testers. (The API parser is called by the Runner, which is the entry point to the program.) By default, BaseAPIParser is used. If you use a custom trainer, the code will try to use API<name_of_your_trainer> , and if that doesn't exist, it will fall back to BaseAPIParser . However, if you explicitly set the api_parser option, then the one you specify will be used. Default yaml: api_parser : null Example command line modification: --api_parser { YourCustomAPIParser: {}}","title":"api_parser"},{"location":"configs/config_general/#trainer","text":"The trainer trains your model. Default yaml: trainer : MetricLossOnly : iterations_per_epoch : 100 dataloader_num_workers : 2 batch_size : 32 freeze_trunk_batchnorm : True label_hierarchy_level : 0 loss_weights : null set_min_label_to_zero : True Example command line modification: # Swap in a different trainer, but keep the input parameters the same --trainer~SWAP~1 { CascadedEmbeddings: null }","title":"trainer"},{"location":"configs/config_general/#num_epochs_train","text":"The maximum number of epochs to train for. Default yaml: num_epochs_train : 1000 Example command line modification: --num_epochs_train 100","title":"num_epochs_train"},{"location":"configs/config_general/#save_interval","text":"Models will be evaluated and saved every save_interval epochs. Default yaml: save_interval : 2 Example command line modification: --save_interval 10","title":"save_interval"},{"location":"configs/config_general/#patience","text":"Training will end if the validation accuracy stops improving after patience+1 epochs. Default yaml: save_interval : 2 Example command line modification: # Don't use patience at all --patience null","title":"patience"},{"location":"configs/config_general/#check_untrained_accuracy","text":"If True , then the tester will compute accuracy for the initial trunk (epoch -1) and initial trunk + embedder (epoch 0). Otherwise, these will be skipped. Default yaml: check_untrained_accuracy : True Example command line modification: --check_untrained_accuracy False","title":"check_untrained_accuracy"},{"location":"configs/config_general/#skip_eval_if_already_done","text":"If True , then the tester will skip evaluation if a split/epoch has already been logged in the log files. If False , then the tester will evaluate a split/epoch regardless of whether it has already been done in the past. Previous logs will be preserved, hence the logs will contain duplicate results, and the most recent version for any split/epoch will be considered the \"official\" value for that split/epoch. Default yaml: skip_eval_if_already_done : True Example command line modification: --skip_eval_if_already_done False","title":"skip_eval_if_already_done"},{"location":"configs/config_general/#skip_ensemble_eval_if_already_done","text":"The same as skip_eval_if_already_done , but for ensembles. Default yaml: skip_ensemble_eval_if_already_done : True Example command line modification: --skip_ensemble_eval_if_already_done False","title":"skip_ensemble_eval_if_already_done"},{"location":"configs/config_general/#log_data_to_tensorboard","text":"Set to False if you don't want to log data to tensorboard. You might want to do this if your disk I/O is slow. Default yaml: log_data_to_tensorboard : True Example command line modification: --log_data_to_tensorboard False","title":"log_data_to_tensorboard"},{"location":"configs/config_general/#save_figures_on_tensorboard","text":"Use matplotlib to plot things on tensorboard. (Most data doesn't require matplotlib.) Default yaml: save_figures_on_tensorboard : False Example command line modification: --save_figures_on_tensorboard True","title":"save_figures_on_tensorboard"},{"location":"configs/config_general/#save_lists_in_db","text":"In record-keeper, non-scalar values are saved in the database as json-lists. This setting is False by default, because these lists can sometimes be quite large, causing the database file size to grow quickly. Default yaml: save_lists_in_db : False Example command line modification: --save_lists_in_db True","title":"save_lists_in_db"},{"location":"configs/config_general/#override_required_compatible_factories","text":"Each APIParser comes with predefined compatible factories, which are used by default, regardless of what is specified in the factories config option. This allows you to specify a trainer without having to specify all the required factories. However, if you have your own custom factory that you know is compatible, and want to use that instead, you should set this flag to True. Default yaml: override_required_compatible_factories : False Example command line modification: --override_required_compatible_factories True","title":"override_required_compatible_factories"},{"location":"configs/config_loss_and_miners/","text":"config_loss_and_miners \u00b6 loss_funcs \u00b6 The loss functions are given embeddings and labels, and output a value on which back propagation can be performed. This config option is a mapping from strings to loss classes. The strings should match the loss names used by your trainer. Default yaml: loss_funcs : metric_loss : ContrastiveLoss : Example command line modification: # Use a different loss function --loss_funcs { metric_loss~OVERRIDE~: { MultiSimilarityLoss: { alpha: 0 .1, beta: 40 , base: 0 .5 }}} sampler \u00b6 The sampler is passed to the PyTorch dataloader, and determines how batches are formed. Use {} if you want random sampling. Default yaml: sampler : MPerClassSampler : m : 4 Example command line modification: # Use random sampling --sampler~OVERRIDE~ {} mining_funcs \u00b6 Mining functions determine the best tuples to train on, within an arbitrarily formed batch. This config option is a mapping from strings to miner classes. The strings should match the miner names used by your trainer. Default yaml: mining_funcs : {} Example command line modification: # Use a miner --mining_funcs { tuple_miner: { MultiSimilarityMiner: { epsilon: 0 .1 }}}","title":"config_loss_and_miners"},{"location":"configs/config_loss_and_miners/#config_loss_and_miners","text":"","title":"config_loss_and_miners"},{"location":"configs/config_loss_and_miners/#loss_funcs","text":"The loss functions are given embeddings and labels, and output a value on which back propagation can be performed. This config option is a mapping from strings to loss classes. The strings should match the loss names used by your trainer. Default yaml: loss_funcs : metric_loss : ContrastiveLoss : Example command line modification: # Use a different loss function --loss_funcs { metric_loss~OVERRIDE~: { MultiSimilarityLoss: { alpha: 0 .1, beta: 40 , base: 0 .5 }}}","title":"loss_funcs"},{"location":"configs/config_loss_and_miners/#sampler","text":"The sampler is passed to the PyTorch dataloader, and determines how batches are formed. Use {} if you want random sampling. Default yaml: sampler : MPerClassSampler : m : 4 Example command line modification: # Use random sampling --sampler~OVERRIDE~ {}","title":"sampler"},{"location":"configs/config_loss_and_miners/#mining_funcs","text":"Mining functions determine the best tuples to train on, within an arbitrarily formed batch. This config option is a mapping from strings to miner classes. The strings should match the miner names used by your trainer. Default yaml: mining_funcs : {} Example command line modification: # Use a miner --mining_funcs { tuple_miner: { MultiSimilarityMiner: { epsilon: 0 .1 }}}","title":"mining_funcs"},{"location":"configs/config_models/","text":"config_models \u00b6 models \u00b6 The models take in input (like images, text etc.) and output embeddings. There is no specific requires about what the structure of the trunk and embedder. The only requirement is that the trunk's output can be fed into the embedder. For example, if you want to use the bninception model, but don't want to append any layers after it, you can set embedder to Identity . This will make the embedder's output equal to its input. Default yaml: models : trunk : bninception : pretrained : imagenet embedder : MLP : layer_sizes : - 128 Example command line modification: # Set embedder to Identity. --models { embedder~OVERRIDE~: { Identity: {}}} \\ # You'll need to delete the embedder_optimizer, because Identity() has no parameters --optimizers { embedder_optimizer~DELETE~: null }","title":"config_models"},{"location":"configs/config_models/#config_models","text":"","title":"config_models"},{"location":"configs/config_models/#models","text":"The models take in input (like images, text etc.) and output embeddings. There is no specific requires about what the structure of the trunk and embedder. The only requirement is that the trunk's output can be fed into the embedder. For example, if you want to use the bninception model, but don't want to append any layers after it, you can set embedder to Identity . This will make the embedder's output equal to its input. Default yaml: models : trunk : bninception : pretrained : imagenet embedder : MLP : layer_sizes : - 128 Example command line modification: # Set embedder to Identity. --models { embedder~OVERRIDE~: { Identity: {}}} \\ # You'll need to delete the embedder_optimizer, because Identity() has no parameters --optimizers { embedder_optimizer~DELETE~: null }","title":"models"},{"location":"configs/config_optimizers/","text":"config_optimizers \u00b6 optimizers \u00b6 Optimizers determine how your model weights are updated. This config option maps from strings to optimizer classes. Each string should have the form <model_name>_optimizer . Default yaml: optimizers : trunk_optimizer : RMSprop : lr : 0.000001 weight_decay : 0.0001 momentum : 0.9 embedder_optimizer : RMSprop : lr : 0.000001 weight_decay : 0.0001 momentum : 0.9 Example command line modification: # Change the learning rate to 0.01, for both the trunk and embedder --optimizers~APPLY~3 { lr: 0 .01 }","title":"config_optimizers"},{"location":"configs/config_optimizers/#config_optimizers","text":"","title":"config_optimizers"},{"location":"configs/config_optimizers/#optimizers","text":"Optimizers determine how your model weights are updated. This config option maps from strings to optimizer classes. Each string should have the form <model_name>_optimizer . Default yaml: optimizers : trunk_optimizer : RMSprop : lr : 0.000001 weight_decay : 0.0001 momentum : 0.9 embedder_optimizer : RMSprop : lr : 0.000001 weight_decay : 0.0001 momentum : 0.9 Example command line modification: # Change the learning rate to 0.01, for both the trunk and embedder --optimizers~APPLY~3 { lr: 0 .01 }","title":"optimizers"},{"location":"configs/config_transforms/","text":"config_transforms \u00b6 transforms \u00b6 Specifies the transforms to be used during training and during evaluation. ToTensor() and Normalize do not need to be specified, as they are added by default. Default yaml: transforms : train : Resize : size : 256 RandomResizedCrop : scale : 0.16 1 ratio : 0.75 1.33 size : 227 RandomHorizontalFlip : p : 0.5 eval : Resize : size : 256 CenterCrop : size : 227 Example command line modification: # Use RandomVerticalFlip instead of RandomHorizontalFlip --transforms { train~SWAP~1: { RandomHorizontalFlip: RandomVerticalFlip }}","title":"config_transforms"},{"location":"configs/config_transforms/#config_transforms","text":"","title":"config_transforms"},{"location":"configs/config_transforms/#transforms","text":"Specifies the transforms to be used during training and during evaluation. ToTensor() and Normalize do not need to be specified, as they are added by default. Default yaml: transforms : train : Resize : size : 256 RandomResizedCrop : scale : 0.16 1 ratio : 0.75 1.33 size : 227 RandomHorizontalFlip : p : 0.5 eval : Resize : size : 256 CenterCrop : size : 227 Example command line modification: # Use RandomVerticalFlip instead of RandomHorizontalFlip --transforms { train~SWAP~1: { RandomHorizontalFlip: RandomVerticalFlip }}","title":"transforms"},{"location":"papers/mlrc/","text":"A Metric Learning Reality Check \u00b6 This page contains additional information for the ECCV 2020 paper by Musgrave et al. Optimization plots \u00b6 Click on the links below to view the bayesian optimization plots. These are also available in the benchmark spreadsheet . The plots were generated using the Ax package. CUB200 Cars196 SOP CUB200 with Batch 256 Contrastive Contrastive Contrastive Contrastive Triplet Triplet Triplet Triplet NTXent NTXent NTXent NTXent ProxyNCA ProxyNCA ProxyNCA ProxyNCA Margin Margin Margin Margin Margin / class Margin / class Margin / class Margin / class Normalized Softmax Normalized Softmax Normalized Softmax Normalized Softmax CosFace CosFace CosFace CosFace ArcFace ArcFace ArcFace ArcFace FastAP FastAP FastAP FastAP SNR Contrastive SNR Contrastive SNR Contrastive SNR Contrastive Multi Similarity Multi Similarity Multi Similarity Multi Similarity Multi Similarity + Miner Multi Similarity + Miner Multi Similarity + Miner Multi Similarity + Miner SoftTriple SoftTriple SoftTriple SoftTriple Optimal hyperparameters \u00b6 The values below are also available in the benchmark spreadsheet . Loss function CUB200 Cars196 SOP CUB200 with Batch 256 Contrastive pos_margin neg_margin -0.2000 1 0.3841 0.2652 0.5409 0.2850 0.5130 0.2227 0.7694 Triplet margin 0.0961 0.1190 0.0451 0.1368 NTXent temperature 0.0091 0.0219 0.0002 0.0415 ProxyNCA proxy lr softmax_scale 6.04e-3 13.98 4.43e-3 7.97 5.28e-4 10.73 2.16e-1 10.03 Margin beta lr margin init beta 1.31e-3 0.0878 0.7838 1.11e-4 0.0781 1.3164 1.82e-3 0.0915 1.1072 1.00e-6 0.0674 0.9762 Margin / class beta lr margin init beta 2.65e-4 0.0779 0.9796 4.76e-05 0.0776 0.9598 7.10e-05 0.0518 0.8424 1.32e-2 -0.0204 0.1097 Normalized Softmax weights lr temperature 4.46e-3 0.1087 1.10e-2 0.0886 5.46e-4 0.0630 7.20e-2 0.0707 CosFace weights lr margin scale 2.53e-3 0.6182 100.0 7.41e-3 0.4324 161.5 2.16e-3 0.3364 100.0 3.99e-3 0.4144 88.23 ArcFace weights lr margin scale 5.13e-3 23.22 100.0 7.39e-06 20.52 49.50 2.01e-3 18.63 220.3 3.95e-2 23.14 78.86 FastAP num_bins 17 27 16 86 SNR Contrastive pos_margin neg_margin regularizer_weight 0.3264 0.8446 0.1382 0.1670 0.9337 0 0.3759 1.0831 0 0.1182 0.6822 0.4744 Multi Similarity alpha beta base 0.01 50.60 0.56 14.35 75.83 0.66 8.49 57.38 0.41 0.01 46.85 0.82 Multi Similarity + Miner alpha beta base epsilon 17.97 75.66 0.77 0.39 7.49 47.99 0.63 0.72 15.94 156.61 0.72 0.34 11.63 55.20 0.85 0.42 SoftTriple weights lr la gamma reg_weight margin 5.37e-05 78.02 58.95 0.3754 0.4307 1.40e-4 17.69 19.18 0.0669 0.3588 8.68e-05 100.00 47.90 N/A 0.3145 1.06e-4 72.12 51.07 0.4430 0.6959 Examples of unfair comparisons in metric learning papers \u00b6 Papers that use a better architecture than their competitors, but don\u2019t disclose it \u00b6 Sampling Matters in Deep Embedding Learning (ICCV 2017) Uses ResNet50, but all competitors use GoogleNet Deep Metric Learning with Hierarchical Triplet Loss (ECCV 2018) Uses BN-Inception, but all competitors use GoogleNet Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning (CVPR 2019) Uses BN-Inception. Claims better performance than ensemble methods, but the ensemble methods use GoogleNet. Deep Metric Learning to Rank (CVPR 2019) Uses ResNet50. In their SOP table, only 1 out of 11 competitor methods use ResNet50. All others use BN-Inception or GoogleNet. Claims better performance than ensemble methods, but the ensemble methods use GoogleNet. Divide and Conquer the Embedding Space for Metric Learning (CVPR 2019) Uses ResNet50. In their Cars196 and SOP tables, only 1 out of 15 competitor methods use ResNet50. The rest use GoogleNet or BN-Inception. The same is true for their CUB200 results, but in that table, they re-implement two of the competitors to use ResNet50. SoftTriple Loss: Deep Metric Learning Without Triplet Sampling (ICCV 2019) Uses BN-Inception. Compares with N-pairs and HDC, but doesn\u2019t mention that these use GoogleNet. They only mention the competitors\u2019 architectures when the competitors use an equal or superior network. Specifically, they mention that the Margin loss uses ResNet50, and HTL uses BN-Inception. Deep Metric Learning with Tuplet Margin Loss (ICCV 2019) Uses ResNet50. In their SOP table, only 1 out of 10 competitors use ResNet50, and in their CUB200 and Cars196 tables, only 1 out of 8 competitors use ResNet50. The rest use GoogleNet or BN-Inception. They also claim better performance than ensemble methods, but the ensemble methods use GoogleNet. Papers that use a higher dimensionality than their competitors, but don\u2019t disclose it \u00b6 Sampling Matters in Deep Embedding Learning (ICCV 2017) Uses size 128. CUB200 table: 4 out of 7 use size 64. Cars196: 4 out of 5 use size 64. SOP: 4 out of 7 use size 64. Deep Metric Learning with Hierarchical Triplet Loss (ECCV 2018) Uses size 512. The top two non-ensemble competitor results use size 384 and 64. Ranked List Loss for Deep Metric Learning (CVPR 2019) Uses size 512 or 1536. For all 3 datasets, 5 out of the 6 competitor results use size 64. Deep Metric Learning with Tuplet Margin Loss (ICCV 2019) Uses size 512. The only competing method that uses the same architecture, uses size 128. Papers that claim to do a simple 256 resize and 227 or 224 random crop, but actually use the more advanced RandomResizedCrop method \u00b6 Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning (CVPR 2019) Link to line in code Divide and Conquer the Embedding Space for Metric Learning (CVPR 2019) Link to line in code MIC: Mining Interclass Characteristics for Improved Metric Learning (ICCV 2019) Link to line in code SoftTriple Loss: Deep Metric Learning Without Triplet Sampling (ICCV 2019) Link to line in code Proxy Anchor Loss for Deep Metric Learning (CVPR 2020) Link to line in code Papers that use a 256 crop size, but whose competitor results use a smaller 227 or 224 size \u00b6 Metric Learning With HORDE: High-Order Regularizer for Deep Embeddings (ICCV 2019) Although they do reimplement some algorithms, and the reimplementations presumably use a crop size of 256, they also compare to paper results that use 227 or 224. Papers that omit details \u00b6 Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning (CVPR 2019) Freezes batchnorm parameters in their code , but this is not mentioned in the paper. Proxy Anchor Loss for Deep Metric Learning (CVPR 2020) Uses the sum of Global Average Pooling (GAP) and Global Max Pooling (GMP) . Competitor papers use just GAP. This is not mentioned in the paper. Examples to back up other claims in section 2.1 \u00b6 \u201cMost papers claim to apply the following transformations: resize the image to 256 x 256, randomly crop to 227 x 227, and do a horizontal flip with 50% chance\u201d. The following papers support this claim \u00b6 Deep Metric Learning via Lifted Structured Feature Embedding (CVPR 2016) Deep Spectral Clustering Learning (ICML 2017) Deep Metric Learning via Facility Location (CVPR 2017) No Fuss Distance Metric Learning using Proxies (ICCV 2017) Deep Metric Learning with Angular Loss (ICCV 2017) Sampling Matters in Deep Embedding Learning (ICCV 2017) Deep Adversarial Metric Learning (CVPR 2018) Classification is a Strong Baseline for Deep Metric Learning (BMVC 2019) Hardness-Aware Deep Metric Learning (CVPR 2019) Deep Asymmetric Metric Learning via Rich Relationship Mining (CVPR 2019) Stochastic Class-based Hard Example Mining for Deep Metric Learning (CVPR 2019) Ranked List Loss for Deep Metric Learning (CVPR 2019) Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning (CVPR 2019) Deep Metric Learning to Rank (CVPR 2019) Divide and Conquer the Embedding Space for Metric Learning (CVPR 2019) MIC: Mining Interclass Characteristics for Improved Metric Learning (ICCV 2019) SoftTriple Loss: Deep Metric Learning Without Triplet Sampling (ICCV 2019) Proxy Anchor Loss for Deep Metric Learning (CVPR 2020) Papers categorized by the optimizer they use \u00b6 SGD: Deep Spectral Clustering Learning (ICML 2017) Deep Metric Learning with Angular Loss (ICCV 2017) Hard-Aware Deeply Cascaded Embedding (ICCV 2017) Deep Metric Learning with Hierarchical Triplet Loss (ECCV 2018) Deep Asymmetric Metric Learning via Rich Relationship Mining (CVPR 2019) Ranked List Loss for Deep Metric Learning (CVPR 2019) Classification is a Strong Baseline for Deep Metric Learning (BMVC 2019) Deep Metric Learning with Tuplet Margin Loss (ICCV 2019) RMSprop: Deep Metric Learning via Facility Location (CVPR 2017) No Fuss Distance Metric Learning using Proxies (ICCV 2017) Adam: Improved Deep Metric Learning with Multi-class N-pair Loss Objective (Neurips 2016) Sampling Matters in Deep Embedding Learning (ICCV 2017) Hybrid-Attention based Decoupled Metric Learning for Zero-Shot Image Retrieval (CVPR 2019) Stochastic Class-based Hard Example Mining for Deep Metric Learning (CVPR 2019) Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning (CVPR 2019) Deep Metric Learning to Rank (CVPR 2019) Divide and Conquer the Embedding Space for Metric Learning (CVPR 2019) SoftTriple Loss: Deep Metric Learning Without Triplet Sampling (ICCV 2019) Metric Learning With HORDE: High-Order Regularizer for Deep Embeddings (ICCV 2019) MIC: Mining Interclass Characteristics for Improved Metric Learning (ICCV 2019) AdamW Proxy Anchor Loss for Deep Metric Learning (CVPR 2020) Papers that do not use confidence intervals \u00b6 All of the previously mentioned papers Papers that do not use a validation set \u00b6 All of the previously mentioned papers What papers report for the contrastive and triplet losses \u00b6 The tables below are what papers have reported for the contrastive and triplet loss, using convnets . We know that the papers are reporting convnet results because they explicitly say so. For example: Lifted Structure Loss : See figures 6, 7, and 12, which indicate that the contrastive and triplet results were obtained using GoogleNet. These results have been cited several times in recent papers. Deep Adversarial Metric Learning : See tables 1, 2, and 3, and this quote from the bottom of page 6 / top of page 7: \"For all the baseline methods and DAML, we employed the same GoogLeNet architecture pre-trained on ImageNet for fair comparisons\" Hardness-Aware Deep Metric Learning : See tables 1, 2, and 3, and this quote from page 8: \"We evaluated all the methods mentioned above using the same pretrained CNN model for fair comparison.\" Reported Precision@1 for the Contrastive Loss \u00b6 Paper CUB200 Cars196 SOP Deep Metric Learning via Lifted Structured Feature Embedding (CVPR 2016) 26.4 21.7 42 Learning Deep Embeddings with Histogram Loss (NIPS 2016) 26.4 N/A 42 Hard-Aware Deeply Cascaded Embedding (ICCV 2017) 26.4 21.7 42 Sampling Matters in Deep Embedding Learning (ICCV 2017) N/A N/A 30.1 Deep Adversarial Metric Learning (CVPR 2018) 27.2 27.6 37.5 Attention-based Ensemble for Deep Metric Learning (ECCV 2018) 26.4 21.7 42 Deep Variational Metric Learning (ECCV 2018) 32.8 35.8 37.4 Classification is a Strong Baseline for Deep Metric Learning (BMVC 2019) 26.4 21.7 42 Deep Asymmetric Metric Learning via Rich Relationship Mining (CVPR 2019) 27.2 27.6 37.5 Hardness-Aware Deep Metric Learning (CVPR 2019) 27.2 27.6 37.5 Metric Learning With HORDE: High-Order Regularizer for Deep Embeddings (ICCV 2019) 55 72.2 N/A Reported Precision@1 for the Triplet Loss \u00b6 Paper CUB200 Cars196 SOP Deep Metric Learning via Lifted Structured Feature Embedding (CVPR 2016) 36.1 39.1 42.1 Learning Deep Embeddings with Histogram Loss (NIPS 2016) 36.1 N/A 42.1 Improved Deep Metric Learning with Multi-class N-pair Loss Objective (NIPS 2016) 43.3 53.84 53.32 Hard-Aware Deeply Cascaded Embedding (ICCV 2017) 36.1 39.1 42.1 Deep Metric Learning with Angular Loss (ICCV 2017) 42.2 45.5 56.5 Deep Adversarial Metric Learning (CVPR 2018) 35.9 45.1 53.9 Deep Variational Metric Learning (ECCV 2018) 39.8 58.5 54.9 Deep Metric Learning with Hierarchical Triplet Loss (ECCV 2018) 55.9 79.2 72.6 Hardness-Aware Deep Metric Learning (CVPR 2019) 35.9 45.1 53.9 Deep Asymmetric Metric Learning via Rich Relationship Mining (CVPR 2019) 35.9 45.1 53.9 Metric Learning With HORDE: High-Order Regularizer for Deep Embeddings (ICCV 2019) 50.5 65.2 N/A Frequently Asked Questions \u00b6 Do you have slides that accompany the paper? \u00b6 Slides are here . Isn't it unfair to fix the model, optimizer, learning rate, and embedding size? \u00b6 Our goal was to compare algorithms fairly. To accomplish this, we used the same network, optimizer, learning rate, image transforms, and embedding dimensionality for each algorithm. There is no theoretical reason why changing any of these parameters would benefit one particular algorithm over the rest. If there is no theoretical reason, then we can only speculate, and if we add hyperparameters based on speculation, then the search space becomes too large to explore. Why did you use BN-Inception? \u00b6 We chose this architecture because it is commonly used in recent metric learning papers. Why was the batch size set to 32 for most of the results? \u00b6 This was done for the sake of computational efficiency. Note that there are: 3 datasets 14 algorithms 50 steps of bayesian optmization 4 fold cross validation This comes to 8400 models to train, which can take a considerable amount of time. Thus, a batch size of 32 made sense. It's also important to remember that there are real-world cases where a large batch size cannot be used. For example, if you want to train on large images, rather than the contrived case of 227x227, then training with a batch size of 32 suddenly makes a lot more sense because you are constrained by GPU memory. So it's reasonable to check the performance of these losses on a batch size of 32. That said, there is a good theoretical reason for a larger batch size benefiting embedding losses more than classification losses. Specifically, embedding losses can benefit from the increased number of pairs/triplets in larger batches. To address this, we benchmarked the 14 methods on CUB200, using a batch size of 256. The results can be found in the supplementary section (the final page) of the paper. Why weren't more hard-mining methods evaluated? \u00b6 We did test one loss+miner combination (Multi-similarity loss + their mining method). But we mainly wanted to do a thorough evaluation of loss functions, because that is the subject of most recent metric learning papers. For the contrastive loss, why is the optimal positive margin a negative value? \u00b6 A negative value should be equivalent to a margin of 0, because the distance between positive pairs cannot be negative, and the margin does not contribute to the gradient. So allowing the hyperparameter optimization to explore negative margins was unnecesary, but by the time I realized this, it wasn't worth changing the optimization bounds. In Figure 2 (papers vs reality) why do you use Precision@1 instead of MAP@R? \u00b6 None of the referenced papers report MAP@R. Since Figure 2a is meant to show reported results, we had to use a metric that was actually reported, i.e. Precision@1. We used the same metric for Figure 2b so that the two graphs could be compared directly side by side. But for the sake of completeness, here's Figure 2b using MAP@R: Reproducing results \u00b6 Install the compatible version \u00b6 Please install version 0.9.32: pip install powerful-benchmarker==0.9.32 Download the experiment folder \u00b6 Download run.py and set the default flags Go to the benchmark spreadsheet Find the experiment you want to reproduce, and click on the link in the \"Config files\" column. You'll see 3 folders: one for CUB, one for Cars, and one for SOP. Open the folder for the dataset you want to train on. Now you'll see several files and folders, one of which ends in \"reproduction0\". Download this folder. (It will include saved models. If you don't want to download the saved models, go into the folder and download just the \"configs\" folder.) Command line scripts \u00b6 Normally reproducing results is as easy as downloading an experiment folder, and using the reproduce_results flag . However, there have been significant changes to the API since these experiments were run, so there are a couple of extra steps required, and they depend on the dataset. Additionally, if you are reproducing an experiment for the Contrastive, Triplet, or SNR Contrastive losses , you have to delete the key/value pair called avg_non_zero_only in the config_loss_and_miners.yaml file. And for the Contrastive loss , you should delete the use_similarity key/value pair in config_loss_and_miners.yaml . In the following code, <experiment_to_reproduce> refers to the folder that contains the configs folder. CUB200: python run.py --reproduce_results <experiment_to_reproduce> \\ --experiment_name <your_experiment_name> \\ --split_manager~SWAP~1 { MLRCSplitManager: {}} \\ --merge_argparse_when_resuming Cars196: python run.py --reproduce_results <experiment_to_reproduce> \\ --experiment_name <your_experiment_name> \\ --config_dataset [ default, with_cars196 ] \\ --config_general [ default, with_cars196 ] \\ --split_manager~SWAP~1 { MLRCSplitManager: {}} \\ --merge_argparse_when_resuming Stanford Online Products python run.py --reproduce_results <experiment_to_reproduce> \\ --experiment_name <your_experiment_name> \\ --config_dataset [ default, with_sop ] \\ --config_general [ default, with_sop ] \\ --split_manager~SWAP~1 { MLRCSplitManager: {}} \\ --merge_argparse_when_resuming CUB200 with batch size 256: python run.py --reproduce_results <experiment_to_reproduce> \\ --experiment_name <your_experiment_name> \\ --config_general [ default, with_256_batch ] \\ --split_manager~SWAP~1 { MLRCSplitManager: {}} \\ --merge_argparse_when_resuming If you don't have the datasets and would like to download them into your dataset_root folder, you can add this flag to the CUB commands: --dataset~OVERRIDE~ { CUB200: { download: True }} Likewise, for the Cars196 and Stanford Online Products commands, replace the --config_dataset flag with: --dataset~OVERRIDE~ { Cars196: { download: True }} or --dataset~OVERRIDE~ { StanfordOnlineProducts: { download: True }} Run evaluation on the test set \u00b6 After training is done, you can get the \"separate 128-dim\" test set performance: python run.py --experiment_name <your_experiment_name> \\ --evaluate --splits_to_eval [ test ] and the \"concatenated 512-dim\" test set performance: python run.py --experiment_name <your_experiment_name> \\ --evaluate_ensemble --splits_to_eval [ test ] Once evaluation is done, you can go to the meta_logs folder and view the results.","title":"A Metric Learning Reality Check"},{"location":"papers/mlrc/#a-metric-learning-reality-check","text":"This page contains additional information for the ECCV 2020 paper by Musgrave et al.","title":"A Metric Learning Reality Check"},{"location":"papers/mlrc/#optimization-plots","text":"Click on the links below to view the bayesian optimization plots. These are also available in the benchmark spreadsheet . The plots were generated using the Ax package. CUB200 Cars196 SOP CUB200 with Batch 256 Contrastive Contrastive Contrastive Contrastive Triplet Triplet Triplet Triplet NTXent NTXent NTXent NTXent ProxyNCA ProxyNCA ProxyNCA ProxyNCA Margin Margin Margin Margin Margin / class Margin / class Margin / class Margin / class Normalized Softmax Normalized Softmax Normalized Softmax Normalized Softmax CosFace CosFace CosFace CosFace ArcFace ArcFace ArcFace ArcFace FastAP FastAP FastAP FastAP SNR Contrastive SNR Contrastive SNR Contrastive SNR Contrastive Multi Similarity Multi Similarity Multi Similarity Multi Similarity Multi Similarity + Miner Multi Similarity + Miner Multi Similarity + Miner Multi Similarity + Miner SoftTriple SoftTriple SoftTriple SoftTriple","title":"Optimization plots"},{"location":"papers/mlrc/#optimal-hyperparameters","text":"The values below are also available in the benchmark spreadsheet . Loss function CUB200 Cars196 SOP CUB200 with Batch 256 Contrastive pos_margin neg_margin -0.2000 1 0.3841 0.2652 0.5409 0.2850 0.5130 0.2227 0.7694 Triplet margin 0.0961 0.1190 0.0451 0.1368 NTXent temperature 0.0091 0.0219 0.0002 0.0415 ProxyNCA proxy lr softmax_scale 6.04e-3 13.98 4.43e-3 7.97 5.28e-4 10.73 2.16e-1 10.03 Margin beta lr margin init beta 1.31e-3 0.0878 0.7838 1.11e-4 0.0781 1.3164 1.82e-3 0.0915 1.1072 1.00e-6 0.0674 0.9762 Margin / class beta lr margin init beta 2.65e-4 0.0779 0.9796 4.76e-05 0.0776 0.9598 7.10e-05 0.0518 0.8424 1.32e-2 -0.0204 0.1097 Normalized Softmax weights lr temperature 4.46e-3 0.1087 1.10e-2 0.0886 5.46e-4 0.0630 7.20e-2 0.0707 CosFace weights lr margin scale 2.53e-3 0.6182 100.0 7.41e-3 0.4324 161.5 2.16e-3 0.3364 100.0 3.99e-3 0.4144 88.23 ArcFace weights lr margin scale 5.13e-3 23.22 100.0 7.39e-06 20.52 49.50 2.01e-3 18.63 220.3 3.95e-2 23.14 78.86 FastAP num_bins 17 27 16 86 SNR Contrastive pos_margin neg_margin regularizer_weight 0.3264 0.8446 0.1382 0.1670 0.9337 0 0.3759 1.0831 0 0.1182 0.6822 0.4744 Multi Similarity alpha beta base 0.01 50.60 0.56 14.35 75.83 0.66 8.49 57.38 0.41 0.01 46.85 0.82 Multi Similarity + Miner alpha beta base epsilon 17.97 75.66 0.77 0.39 7.49 47.99 0.63 0.72 15.94 156.61 0.72 0.34 11.63 55.20 0.85 0.42 SoftTriple weights lr la gamma reg_weight margin 5.37e-05 78.02 58.95 0.3754 0.4307 1.40e-4 17.69 19.18 0.0669 0.3588 8.68e-05 100.00 47.90 N/A 0.3145 1.06e-4 72.12 51.07 0.4430 0.6959","title":"Optimal hyperparameters"},{"location":"papers/mlrc/#examples-of-unfair-comparisons-in-metric-learning-papers","text":"","title":"Examples of unfair comparisons in metric learning papers"},{"location":"papers/mlrc/#papers-that-use-a-better-architecture-than-their-competitors-but-dont-disclose-it","text":"Sampling Matters in Deep Embedding Learning (ICCV 2017) Uses ResNet50, but all competitors use GoogleNet Deep Metric Learning with Hierarchical Triplet Loss (ECCV 2018) Uses BN-Inception, but all competitors use GoogleNet Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning (CVPR 2019) Uses BN-Inception. Claims better performance than ensemble methods, but the ensemble methods use GoogleNet. Deep Metric Learning to Rank (CVPR 2019) Uses ResNet50. In their SOP table, only 1 out of 11 competitor methods use ResNet50. All others use BN-Inception or GoogleNet. Claims better performance than ensemble methods, but the ensemble methods use GoogleNet. Divide and Conquer the Embedding Space for Metric Learning (CVPR 2019) Uses ResNet50. In their Cars196 and SOP tables, only 1 out of 15 competitor methods use ResNet50. The rest use GoogleNet or BN-Inception. The same is true for their CUB200 results, but in that table, they re-implement two of the competitors to use ResNet50. SoftTriple Loss: Deep Metric Learning Without Triplet Sampling (ICCV 2019) Uses BN-Inception. Compares with N-pairs and HDC, but doesn\u2019t mention that these use GoogleNet. They only mention the competitors\u2019 architectures when the competitors use an equal or superior network. Specifically, they mention that the Margin loss uses ResNet50, and HTL uses BN-Inception. Deep Metric Learning with Tuplet Margin Loss (ICCV 2019) Uses ResNet50. In their SOP table, only 1 out of 10 competitors use ResNet50, and in their CUB200 and Cars196 tables, only 1 out of 8 competitors use ResNet50. The rest use GoogleNet or BN-Inception. They also claim better performance than ensemble methods, but the ensemble methods use GoogleNet.","title":"Papers that use a better architecture than their competitors, but don\u2019t disclose it"},{"location":"papers/mlrc/#papers-that-use-a-higher-dimensionality-than-their-competitors-but-dont-disclose-it","text":"Sampling Matters in Deep Embedding Learning (ICCV 2017) Uses size 128. CUB200 table: 4 out of 7 use size 64. Cars196: 4 out of 5 use size 64. SOP: 4 out of 7 use size 64. Deep Metric Learning with Hierarchical Triplet Loss (ECCV 2018) Uses size 512. The top two non-ensemble competitor results use size 384 and 64. Ranked List Loss for Deep Metric Learning (CVPR 2019) Uses size 512 or 1536. For all 3 datasets, 5 out of the 6 competitor results use size 64. Deep Metric Learning with Tuplet Margin Loss (ICCV 2019) Uses size 512. The only competing method that uses the same architecture, uses size 128.","title":"Papers that use a higher dimensionality than their competitors, but don\u2019t disclose it"},{"location":"papers/mlrc/#papers-that-claim-to-do-a-simple-256-resize-and-227-or-224-random-crop-but-actually-use-the-more-advanced-randomresizedcrop-method","text":"Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning (CVPR 2019) Link to line in code Divide and Conquer the Embedding Space for Metric Learning (CVPR 2019) Link to line in code MIC: Mining Interclass Characteristics for Improved Metric Learning (ICCV 2019) Link to line in code SoftTriple Loss: Deep Metric Learning Without Triplet Sampling (ICCV 2019) Link to line in code Proxy Anchor Loss for Deep Metric Learning (CVPR 2020) Link to line in code","title":"Papers that claim to do a simple 256 resize and 227 or 224 random crop, but actually use the more advanced RandomResizedCrop method"},{"location":"papers/mlrc/#papers-that-use-a-256-crop-size-but-whose-competitor-results-use-a-smaller-227-or-224-size","text":"Metric Learning With HORDE: High-Order Regularizer for Deep Embeddings (ICCV 2019) Although they do reimplement some algorithms, and the reimplementations presumably use a crop size of 256, they also compare to paper results that use 227 or 224.","title":"Papers that use a 256 crop size, but whose competitor results use a smaller 227 or 224 size"},{"location":"papers/mlrc/#papers-that-omit-details","text":"Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning (CVPR 2019) Freezes batchnorm parameters in their code , but this is not mentioned in the paper. Proxy Anchor Loss for Deep Metric Learning (CVPR 2020) Uses the sum of Global Average Pooling (GAP) and Global Max Pooling (GMP) . Competitor papers use just GAP. This is not mentioned in the paper.","title":"Papers that omit details"},{"location":"papers/mlrc/#examples-to-back-up-other-claims-in-section-21","text":"","title":"Examples to back up other claims in section 2.1"},{"location":"papers/mlrc/#most-papers-claim-to-apply-the-following-transformations-resize-the-image-to-256-x-256-randomly-crop-to-227-x-227-and-do-a-horizontal-flip-with-50-chance-the-following-papers-support-this-claim","text":"Deep Metric Learning via Lifted Structured Feature Embedding (CVPR 2016) Deep Spectral Clustering Learning (ICML 2017) Deep Metric Learning via Facility Location (CVPR 2017) No Fuss Distance Metric Learning using Proxies (ICCV 2017) Deep Metric Learning with Angular Loss (ICCV 2017) Sampling Matters in Deep Embedding Learning (ICCV 2017) Deep Adversarial Metric Learning (CVPR 2018) Classification is a Strong Baseline for Deep Metric Learning (BMVC 2019) Hardness-Aware Deep Metric Learning (CVPR 2019) Deep Asymmetric Metric Learning via Rich Relationship Mining (CVPR 2019) Stochastic Class-based Hard Example Mining for Deep Metric Learning (CVPR 2019) Ranked List Loss for Deep Metric Learning (CVPR 2019) Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning (CVPR 2019) Deep Metric Learning to Rank (CVPR 2019) Divide and Conquer the Embedding Space for Metric Learning (CVPR 2019) MIC: Mining Interclass Characteristics for Improved Metric Learning (ICCV 2019) SoftTriple Loss: Deep Metric Learning Without Triplet Sampling (ICCV 2019) Proxy Anchor Loss for Deep Metric Learning (CVPR 2020)","title":"\u201cMost papers claim to apply the following transformations: resize the image to 256 x 256, randomly crop to 227 x 227, and do a horizontal flip with 50% chance\u201d. The following papers support this claim"},{"location":"papers/mlrc/#papers-categorized-by-the-optimizer-they-use","text":"SGD: Deep Spectral Clustering Learning (ICML 2017) Deep Metric Learning with Angular Loss (ICCV 2017) Hard-Aware Deeply Cascaded Embedding (ICCV 2017) Deep Metric Learning with Hierarchical Triplet Loss (ECCV 2018) Deep Asymmetric Metric Learning via Rich Relationship Mining (CVPR 2019) Ranked List Loss for Deep Metric Learning (CVPR 2019) Classification is a Strong Baseline for Deep Metric Learning (BMVC 2019) Deep Metric Learning with Tuplet Margin Loss (ICCV 2019) RMSprop: Deep Metric Learning via Facility Location (CVPR 2017) No Fuss Distance Metric Learning using Proxies (ICCV 2017) Adam: Improved Deep Metric Learning with Multi-class N-pair Loss Objective (Neurips 2016) Sampling Matters in Deep Embedding Learning (ICCV 2017) Hybrid-Attention based Decoupled Metric Learning for Zero-Shot Image Retrieval (CVPR 2019) Stochastic Class-based Hard Example Mining for Deep Metric Learning (CVPR 2019) Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning (CVPR 2019) Deep Metric Learning to Rank (CVPR 2019) Divide and Conquer the Embedding Space for Metric Learning (CVPR 2019) SoftTriple Loss: Deep Metric Learning Without Triplet Sampling (ICCV 2019) Metric Learning With HORDE: High-Order Regularizer for Deep Embeddings (ICCV 2019) MIC: Mining Interclass Characteristics for Improved Metric Learning (ICCV 2019) AdamW Proxy Anchor Loss for Deep Metric Learning (CVPR 2020)","title":"Papers categorized by the optimizer they use"},{"location":"papers/mlrc/#papers-that-do-not-use-confidence-intervals","text":"All of the previously mentioned papers","title":"Papers that do not use confidence intervals"},{"location":"papers/mlrc/#papers-that-do-not-use-a-validation-set","text":"All of the previously mentioned papers","title":"Papers that do not use a validation set"},{"location":"papers/mlrc/#what-papers-report-for-the-contrastive-and-triplet-losses","text":"The tables below are what papers have reported for the contrastive and triplet loss, using convnets . We know that the papers are reporting convnet results because they explicitly say so. For example: Lifted Structure Loss : See figures 6, 7, and 12, which indicate that the contrastive and triplet results were obtained using GoogleNet. These results have been cited several times in recent papers. Deep Adversarial Metric Learning : See tables 1, 2, and 3, and this quote from the bottom of page 6 / top of page 7: \"For all the baseline methods and DAML, we employed the same GoogLeNet architecture pre-trained on ImageNet for fair comparisons\" Hardness-Aware Deep Metric Learning : See tables 1, 2, and 3, and this quote from page 8: \"We evaluated all the methods mentioned above using the same pretrained CNN model for fair comparison.\"","title":"What papers report for the contrastive and triplet losses"},{"location":"papers/mlrc/#reported-precision1-for-the-contrastive-loss","text":"Paper CUB200 Cars196 SOP Deep Metric Learning via Lifted Structured Feature Embedding (CVPR 2016) 26.4 21.7 42 Learning Deep Embeddings with Histogram Loss (NIPS 2016) 26.4 N/A 42 Hard-Aware Deeply Cascaded Embedding (ICCV 2017) 26.4 21.7 42 Sampling Matters in Deep Embedding Learning (ICCV 2017) N/A N/A 30.1 Deep Adversarial Metric Learning (CVPR 2018) 27.2 27.6 37.5 Attention-based Ensemble for Deep Metric Learning (ECCV 2018) 26.4 21.7 42 Deep Variational Metric Learning (ECCV 2018) 32.8 35.8 37.4 Classification is a Strong Baseline for Deep Metric Learning (BMVC 2019) 26.4 21.7 42 Deep Asymmetric Metric Learning via Rich Relationship Mining (CVPR 2019) 27.2 27.6 37.5 Hardness-Aware Deep Metric Learning (CVPR 2019) 27.2 27.6 37.5 Metric Learning With HORDE: High-Order Regularizer for Deep Embeddings (ICCV 2019) 55 72.2 N/A","title":"Reported Precision@1 for the Contrastive Loss"},{"location":"papers/mlrc/#reported-precision1-for-the-triplet-loss","text":"Paper CUB200 Cars196 SOP Deep Metric Learning via Lifted Structured Feature Embedding (CVPR 2016) 36.1 39.1 42.1 Learning Deep Embeddings with Histogram Loss (NIPS 2016) 36.1 N/A 42.1 Improved Deep Metric Learning with Multi-class N-pair Loss Objective (NIPS 2016) 43.3 53.84 53.32 Hard-Aware Deeply Cascaded Embedding (ICCV 2017) 36.1 39.1 42.1 Deep Metric Learning with Angular Loss (ICCV 2017) 42.2 45.5 56.5 Deep Adversarial Metric Learning (CVPR 2018) 35.9 45.1 53.9 Deep Variational Metric Learning (ECCV 2018) 39.8 58.5 54.9 Deep Metric Learning with Hierarchical Triplet Loss (ECCV 2018) 55.9 79.2 72.6 Hardness-Aware Deep Metric Learning (CVPR 2019) 35.9 45.1 53.9 Deep Asymmetric Metric Learning via Rich Relationship Mining (CVPR 2019) 35.9 45.1 53.9 Metric Learning With HORDE: High-Order Regularizer for Deep Embeddings (ICCV 2019) 50.5 65.2 N/A","title":"Reported Precision@1 for the Triplet Loss"},{"location":"papers/mlrc/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"papers/mlrc/#do-you-have-slides-that-accompany-the-paper","text":"Slides are here .","title":"Do you have slides that accompany the paper?"},{"location":"papers/mlrc/#isnt-it-unfair-to-fix-the-model-optimizer-learning-rate-and-embedding-size","text":"Our goal was to compare algorithms fairly. To accomplish this, we used the same network, optimizer, learning rate, image transforms, and embedding dimensionality for each algorithm. There is no theoretical reason why changing any of these parameters would benefit one particular algorithm over the rest. If there is no theoretical reason, then we can only speculate, and if we add hyperparameters based on speculation, then the search space becomes too large to explore.","title":"Isn't it unfair to fix the model, optimizer, learning rate, and embedding size?"},{"location":"papers/mlrc/#why-did-you-use-bn-inception","text":"We chose this architecture because it is commonly used in recent metric learning papers.","title":"Why did you use BN-Inception?"},{"location":"papers/mlrc/#why-was-the-batch-size-set-to-32-for-most-of-the-results","text":"This was done for the sake of computational efficiency. Note that there are: 3 datasets 14 algorithms 50 steps of bayesian optmization 4 fold cross validation This comes to 8400 models to train, which can take a considerable amount of time. Thus, a batch size of 32 made sense. It's also important to remember that there are real-world cases where a large batch size cannot be used. For example, if you want to train on large images, rather than the contrived case of 227x227, then training with a batch size of 32 suddenly makes a lot more sense because you are constrained by GPU memory. So it's reasonable to check the performance of these losses on a batch size of 32. That said, there is a good theoretical reason for a larger batch size benefiting embedding losses more than classification losses. Specifically, embedding losses can benefit from the increased number of pairs/triplets in larger batches. To address this, we benchmarked the 14 methods on CUB200, using a batch size of 256. The results can be found in the supplementary section (the final page) of the paper.","title":"Why was the batch size set to 32 for most of the results?"},{"location":"papers/mlrc/#why-werent-more-hard-mining-methods-evaluated","text":"We did test one loss+miner combination (Multi-similarity loss + their mining method). But we mainly wanted to do a thorough evaluation of loss functions, because that is the subject of most recent metric learning papers.","title":"Why weren't more hard-mining methods evaluated?"},{"location":"papers/mlrc/#for-the-contrastive-loss-why-is-the-optimal-positive-margin-a-negative-value","text":"A negative value should be equivalent to a margin of 0, because the distance between positive pairs cannot be negative, and the margin does not contribute to the gradient. So allowing the hyperparameter optimization to explore negative margins was unnecesary, but by the time I realized this, it wasn't worth changing the optimization bounds.","title":"For the contrastive loss, why is the optimal positive margin a negative value?"},{"location":"papers/mlrc/#in-figure-2-papers-vs-reality-why-do-you-use-precision1-instead-of-mapr","text":"None of the referenced papers report MAP@R. Since Figure 2a is meant to show reported results, we had to use a metric that was actually reported, i.e. Precision@1. We used the same metric for Figure 2b so that the two graphs could be compared directly side by side. But for the sake of completeness, here's Figure 2b using MAP@R:","title":"In Figure 2 (papers vs reality) why do you use Precision@1 instead of MAP@R?"},{"location":"papers/mlrc/#reproducing-results","text":"","title":"Reproducing results"},{"location":"papers/mlrc/#install-the-compatible-version","text":"Please install version 0.9.32: pip install powerful-benchmarker==0.9.32","title":"Install the compatible version"},{"location":"papers/mlrc/#download-the-experiment-folder","text":"Download run.py and set the default flags Go to the benchmark spreadsheet Find the experiment you want to reproduce, and click on the link in the \"Config files\" column. You'll see 3 folders: one for CUB, one for Cars, and one for SOP. Open the folder for the dataset you want to train on. Now you'll see several files and folders, one of which ends in \"reproduction0\". Download this folder. (It will include saved models. If you don't want to download the saved models, go into the folder and download just the \"configs\" folder.)","title":"Download the experiment folder"},{"location":"papers/mlrc/#command-line-scripts","text":"Normally reproducing results is as easy as downloading an experiment folder, and using the reproduce_results flag . However, there have been significant changes to the API since these experiments were run, so there are a couple of extra steps required, and they depend on the dataset. Additionally, if you are reproducing an experiment for the Contrastive, Triplet, or SNR Contrastive losses , you have to delete the key/value pair called avg_non_zero_only in the config_loss_and_miners.yaml file. And for the Contrastive loss , you should delete the use_similarity key/value pair in config_loss_and_miners.yaml . In the following code, <experiment_to_reproduce> refers to the folder that contains the configs folder. CUB200: python run.py --reproduce_results <experiment_to_reproduce> \\ --experiment_name <your_experiment_name> \\ --split_manager~SWAP~1 { MLRCSplitManager: {}} \\ --merge_argparse_when_resuming Cars196: python run.py --reproduce_results <experiment_to_reproduce> \\ --experiment_name <your_experiment_name> \\ --config_dataset [ default, with_cars196 ] \\ --config_general [ default, with_cars196 ] \\ --split_manager~SWAP~1 { MLRCSplitManager: {}} \\ --merge_argparse_when_resuming Stanford Online Products python run.py --reproduce_results <experiment_to_reproduce> \\ --experiment_name <your_experiment_name> \\ --config_dataset [ default, with_sop ] \\ --config_general [ default, with_sop ] \\ --split_manager~SWAP~1 { MLRCSplitManager: {}} \\ --merge_argparse_when_resuming CUB200 with batch size 256: python run.py --reproduce_results <experiment_to_reproduce> \\ --experiment_name <your_experiment_name> \\ --config_general [ default, with_256_batch ] \\ --split_manager~SWAP~1 { MLRCSplitManager: {}} \\ --merge_argparse_when_resuming If you don't have the datasets and would like to download them into your dataset_root folder, you can add this flag to the CUB commands: --dataset~OVERRIDE~ { CUB200: { download: True }} Likewise, for the Cars196 and Stanford Online Products commands, replace the --config_dataset flag with: --dataset~OVERRIDE~ { Cars196: { download: True }} or --dataset~OVERRIDE~ { StanfordOnlineProducts: { download: True }}","title":"Command line scripts"},{"location":"papers/mlrc/#run-evaluation-on-the-test-set","text":"After training is done, you can get the \"separate 128-dim\" test set performance: python run.py --experiment_name <your_experiment_name> \\ --evaluate --splits_to_eval [ test ] and the \"concatenated 512-dim\" test set performance: python run.py --experiment_name <your_experiment_name> \\ --evaluate_ensemble --splits_to_eval [ test ] Once evaluation is done, you can go to the meta_logs folder and view the results.","title":"Run evaluation on the test set"}]}